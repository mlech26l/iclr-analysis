{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper makes a significant attempt at solving one of the practical problems in machine learning -- learning from many noisy and limited number of clean labels. This setting is presumably more practical than the setting of few-shot learning. Noisy labels are often abundantly available and investing in methods that can take the noise into account for building a discriminative model is quite timely. \n\nTo be honest, the theoretical contribution of the paper is limited.  The authors make use of the nearest neighbour graph obtained from a reduced-dimensional set of features to compute the weights of the noisy labels that must guide the predictive model. From this perspective, the paper seems like an application of existing tools (such as CNN, graph convolutional network and binary classification). However, that does not undermine the superior results the authors have received in the novel application they have targeted. I appreciate the effort that went validating these ideas with real-world datasets.\n\nIn future, I would like to see a joint approach to such training, where the function g(), the nearest neighbour graph loss and the classification loss are all tied in the same objective function and are optimized jointly. \n\nThe paper has few really minor grammatical errors and typos. Please fix those before uploading the final draft. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}