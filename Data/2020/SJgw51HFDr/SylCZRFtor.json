{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "I am the emergency reviewer. Sorry for the late.\n\nThis paper studies a very interesting topic: eliminating small magnitude components of weight and activation vector instead of eliminating small magnitude components of gradients. A clear interpretation and definition towards the forward and backward propagation is presented. The difference between meProp versus SWAT is also plain. Based on some experiment results shown in Figure2, authors announced that accuracy is extremely sensitive to sparsification of output gradients. Thus algorithms SWAT and SAW are proposed to prune the model, which are respectively training with sparse weights and activations, and SWAT only sparsifies the backward pass. Top-K selection is implemented to select which components are set to zero during sparsification.\n\nStrengths:\n1. The writing logic ascends step by step.\n2. Authors showed the harmfulness of the sparsity of gradients by experiment results. Also the comparison between the sparsity of weights and activations are meaningful.\n3. Sufficient experiments are done to generalize SWAT to different models, and the results are fascinating on ImageNet.\n\nWeaknesses: \nIt's a borderline paper. \n1. lack of novelty. The paper has shown a lot experiment results on basic models, but the raising of Top-K algorithm is not novel. Why it is Top-K but not other metrics for selecting zero components? In this view, the paper is likely to be a project summary.\n2. Less comparison to other basic pruning models. More experiments should be done to compare SWAT with other sota pruning models. Then the results will be convincing.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}