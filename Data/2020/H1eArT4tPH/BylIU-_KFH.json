{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The paper studies the mean and variance of the gradient norm at each layer for vanilla feedforward, ResNet and DenseNet, respectively, at the initialization step, which is related with Hanin & Ronick 2018 studying the mean and variance of forward activations. They show that ResNet and DenseNet preserve the variance of the layer gradient norm through depths. In comparison, for the vanilla feedforward network, although the mean of the gradient norm is preserved if  is properly initialized, the variance of the layer gradient norm increases over depths, which may explode or decay the gradient at deeper layers. \n\nThe result presented in the paper is interesting, and the theory and empirical verification have a good match. I  recommend the acceptance after the following points are well addressed.\n\n1. The mean and variance of layer activation norm and gradient norm have been studied in the mean field literatures for example [1] and the paper does not have a good comparison with them.\n2. The experiment of  CONCATENATIVE RELU is not convincing given the small tasks intertwined performance.\n\nMinor presentation flaws:\n1. The sentence in Abstract \"This depth invariant result is surprising in light of the literature results that state that the norm of the layer\u2019s activations grows exponentially with the specific layer\u2019s depth.\" is not clear itself because of no relevant reference.\n2. There are many typos such as \"weather\", \"for of\" in the paper. Please carefully correct them.\n3. The values on the y-axis of Figure 2 and Figure3 are not correctly shown.\n\n[1] Greg Yang and Samuel Schoenholz 2017. Mean Field Residual Networks: On the Edge of Chaos\n\n\n#####after rebuttal period, I found a fatal error in the paper#####\n\nIn (13), the network output $y_t^L$ is decomposed into two parts $y^{L,k}_t$ and $\\hat{y}^{L,k}_t$, where $y^{L,k}_t$ is composed of paths that go through weight matrix $W^k$ and $\\hat{y}^{L,k}_t$ is composed of paths that skip weight matrix $W^k$.\n\nIn (14), the paper derives $J^k:=\\frac{\\partial y_t^L}{\\partial W^k} = \\frac{\\partial y^{L,k}_t}{\\partial W^k}$. However, in fact the other part $\\hat{y}^{L,k}_t$ also has gradient with respect to  $W^k$ even if the path $\\gamma_t$ does not go through $W^k$ because the activation $z_{\\gamma_t}$ is a function of  $W^k$ .\n\nWith this error, all the following claims in the paper are wrong. Thus I vote to reject this paper.  \n\nOne can do experiment to verify  that \"Thm. 1 also reveals a surprising property of the gradients in general Relu networks. That is, when the weights are sampled from the same distribution in each layer, the gradient\u2019s magnitude for each layer are equal in expectation, and depend only on the output statistics.\" is wrong for ResNet.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}