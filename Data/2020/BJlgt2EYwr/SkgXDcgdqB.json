{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "*UPDATE* I have read the other reviews, author's comments and the revised pdf. I maintain my weak accept rating, the paper is borderline but above the bar. The inclusion of experiments that Reviewer1 suggested definitely make the contributions stronger. I believe the paper will be substantially stronger with a careful study of where the empirical improvement is coming from. The theory (that the approximate gradient has an acute angle with the desired gradient) is potentially vulnerable -- this property is certainly desirable when stochastic-optimizing convex functions (with appropriate step sizes) but it's not trivial that it gives good behavior for optimizing non-convex functions like NAS. Without this careful study, it is not obvious that the proposed method doesn't suffer from it's own \"gradient traps\". That said, pointing out gradient approximation issues with differentiable NAS may be a valuable enough contribution.\n\nThe paper studies differentiable approaches to neural architecture search and convincingly points out that existing approximations to the gradient w.r.t. architecture parameters are problematic. A new approximation is proposed, and evaluated to show that degenerate architectures are not getting selected once search has converged (empirically) on standard image classification datasets.\nThe problem with existing approximations (e.g. first-order or second-order DARTS) is explained clearly. It is however unclear whether the proposed solution provides a complete solution, or if there are avenues for further improvement. The key question marks are: is the proposal a tractable approximation? is the empirical improvement indeed arising because of better gradients? Studying these two questions carefully via experiment will make the paper's contributions stronger.\n\nMinor questions (related to writing/exposition):\nHow is Eqn2 different from Eq3? If they are the same, please remove the redundant equation (you already repeat it in the Introduction).\nWhy is Eqn7 a tractable approximation when H is very high-dimensional? I agree that it is more efficient that H^-1, but don't see how it can be tractable to compute in general.\nSection 4.1.2: What is \"auxiliary loss tower\"?\n\nAbstract: \"obstacles it\" (obstacles is an awkward verb, perhaps use \"hinders it\" instead)\nIntroduction: \"was very few covered in previous works\" is an awkward phrase. \"has not been studied carefully in previous works\"\nRelated work: \"exhausted search\" -> \"exhaustive search\"\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}