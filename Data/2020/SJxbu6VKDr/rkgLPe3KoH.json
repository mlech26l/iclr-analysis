{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The core of the paper is around a block to gate cnn outputs between layers controlled by a global context. The outputs of the block are combined with the original cnn outputs via a residual connection.\n\nNone of the ideas is particularly new, but the main contribution seems to be to show that we can still get benefits in various tasks by using a very simple setup with l2 global norm per channel for the global pooling context scaled by simple linear transformations per cnn output channel + tanh activation for the gating function. By itself I think this is a good practical finding and good tinkering results should always be welcomed in my opinion.\n\nHowever, had the paper been more succinct, with less attempts at interpretation and without trying to present a fundamentally new unit (we have too many acronyms already), I'd be more open to accept it. Sentences like \"This capability is more consistent with the training process in biological neural networks...\" in section 3  and various attempts at interpretation (as the whole section 4.4) come across as a little pretentious and only obfuscate the main finding in my opinion. \n\nThe experiments are pretty extensive, and I believe just studying the effects of adding global pooling contexts between the layers on the various tasks is interesting by itself.\n\nSome minor comments on presentation:\n\nFigure 1: It would be clearer not to include the residual connection inside the building block (i.e., just tanh and not (1+ tanh). Same for \"Gating Adaptation\" on Section 3. I would just highlight that the proposed gating block is combined with the original output with residual connections.\n\nA short segment of code (as reviewer #3 points out) would make it immediately obvious what is being proposed and highlight the simplicity of it.\n\nConclusion and Future Work: \"We conduct expensive\" -> \"We conduct extensive\" ?\n\nAppendix. Experiments on CIFAR seem unnecessary at this point, and I don't think the visualizations (and accompanying interpretations) add much to the paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}