{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper provides a multi-level graph-coarsening approach that can improve the predictive and computational performances of numerous existing unsupervised graph embedding models. The proposed approach is a pipeline consisting of 4 steps, viz: 1) Graph Fusion - that fuses attribute similarity graph with network topology, 2> Graph Coarsening - that reduces the graph size iteratively, 3> Graph embedding - using existing models and 4> Embedding refinement. While such a pipeline for scaling using a graph coarsening and refinement based approach is not new, the authors have carefully designed the pipeline to be effective and be scalable such as without any costly learning components (as in mile). The effectiveness of the proposed approach is evaluated with the node classification task on 6 datasets.\n\n\nStrengths:\n- The paper addresses a very important problem. The paper proposes a well-designed pipeline to scale existing embedding models.\n- Experimental results support that the proposed approach is effective, especially in terms of reducing computation complexity. \n\nWeaknesses:\n- While the experimental results are convincing on the computation front, I have few concerns on the performance front. \n   a) 'MILE with the fused graph' baseline is missing. It can been seen from Figure 3 that the incorporation of the attribute graph provides a significant performance benefit. Thus it is necessary to have this baseline to understand the improvement gap w.r.t to MILE. I believe this is a fair comparison to make as the graph fusion component is a commonly used technique in the last decade.\n  b) Improvements are inconclusive without additional results on other standard non-attributed graph datasets. In Figure 3, ignoring the model with the fused graph, MILE seems to be comparable to GraphZoom overall. As with the existing results, it's not conclusive whether GraphZoom is better than MILE. Also, add variance and report t-test results. \n  c) That said, it can be seen from Figure 2, that GraphZoom significantly outperforms both DW and MILE(DW) on a large non-attributed dataset. However, it is not clear where the significant increase in performance benefits stems from. More analysis is required here.\n- Results on other unsupervised embedding task missings. It is important to evaluate the embeddings additionally for the link prediction task at the least. \n\nAdditional comment:\n- It would be helpful to incorporate one if not some of the attributed graph embedding model as a base model and baseline, such as Deep Graph Infomax (DGI). \n- It should be easy to use a mini-batch version of GCN with MILE and use it for inductive learning. \n- It would interesting to see what the performance will be without the refinement step. \n\nIf my concerns regarding the experiments are positively addressed, I'm willing to improve the score. \n\n-----------------\nAfter the rebuttal, I have updated my score from 3 to 8 as the authors have satisfactorily responded to the concerns raised. \n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}