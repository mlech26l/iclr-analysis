{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes a new method for improving generative properties of VAE model. The idea is to train VAE in two stages: at first, train the vanilla VAE, then at the second stage freeze the encoder part and train the decoder part as a GAN generator with an additional regularizer which encourages cycle consistency in the latent space. Also the authors claim that other VAE-GAN hybrids which try to improve VAE model are \u201cmisguided\u201d and poor samples and reconstructions of VAE are the consequence of minimum description length problem. \n\nConcerns:\n1) The main concern about this paper is the inaccuracy and very general statements without theoretical or empirical justification. For example, the authors claim that \u201cthe whole point of VAEs is to capture only compressible information and discard information specific to any particular image\u201d. What is the definition of \u201conly compressible information\u201d or \u201cinformation specific to any particular image\u201d? Is there an experiment which can support this statement? Other examples of such general statements: \u201cstrength of a VAE is that it builds a model of the dataset that does not over-fit\u201d, \u201cthe latent code does not contain enough information to do the reconstruction\u201d, \u201cVAEs are not broken and \u201cfixing\u201d them is actually likely to break them\u201d. \n2) Poor experiment comparisons with other baselines. The authors compare their method only with the vanilla VAE which is clearly insufficient. The authors claim that other VAE-GAN hybrids break the \u201cstrength of the VAE\u201d. Could you please provide examples of problems and provide experiments where VAE-GAN baselines will be worse than VAE or the proposed method?\n3) The paper structure is very confusing. The main part and experiments part are mixed. Therefore, it is hard to follow the text. The experiment setup is not clearly stated. For example, it is unclear for which dataset luminance-normalized Laplacian was computed. \n\nOverall, the paper proposes a new method and  gives an alternative view on the VAE model. However, statements from the paper are very pretentious and are not rigorously proven. Also there are not empirical comparisons with other VAE-GAN hybrids. Therefore, I would suggest rejecting the current version.\n\n--------------------------------------------------------\nUpdate after author rebuttal\n\nThank you for your thoughtful response. However, I still think that the paper does not give new insights about VAE model and has poor experiment justifications of their statements. Considering MDL interpretation of VAE it is not new (see [1]). Therefore, the contribution of this paper is very limited. \n\nAfter reviewing the other reviews and the author rebuttal, I do not change my original score.\n\n[1] Xi Chen et al.,  Variational  Lossy  Autoencoder, 2016\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}