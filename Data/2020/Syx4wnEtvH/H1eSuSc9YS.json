{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "\nIn this paper, the authors made a study on large-batch training for the BERT, and successfully trained a BERT model in 76 minutes. The results look quite exciting, however, after looking into the details of the paper, I would say that this is just a kind of RED AI \u2013 the results were mostly achieved by putting together a huge number of TPUs, without necessary technical innovation and fundamental contributions.\n\n1)\tThe work used 1024 TPUs to achieve 76-min training. If we compare this with the original BERT training (16 TPU for 81 hours), there is no algorithmic speed up at all (only system speedup). Not to mention that making a single BERT training faster by using more resources does not seem to be a big thing \u2013 one can do multiple BERT training experiments in parallel or in pipeline, which will correspond to similar innovation speed.\n\n2)\tThe theoretical analysis is not very impressive, and to certain degree, is not helpful. The theory just says that in certain conditions, both LARS and LAMB converge fasters than SGD. However, LAMB ha no advantage over LARS at all, which cannot well explain the experimental observations. Furthermore, when \\beta_2 > 0, the convergence rate of LAMB is even slower than LARS, which delivers some contradictory message. As we know, \\beta_2>0 is very important, otherwise the optimization algorithm will not be ADAM at all.\n\nOverall speaking, I am afraid that such work do not have sufficient theoretical or algorithmic contributions. And I doubt the true value of adding a huge number of computational resources to achieve speedup. \n\n\n**I read the author rebuttal. Thanks for the clarification on the algorithmic contribution of LAMB. However, my other concerns still remain. I have adjusted my rating by a little, but I can hardly move to the positive side.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}