{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #5", "review": "This paper introduces \u201cstiffness\u201d, a new metric to characterize generalization in neural networks. Stiffness is a pretty simple concept and is relatively straightforward to compute. The authors evaluate this metric on standard datasets using two relatively small neural networks. On the whole, the paper is written clearly and explains its methodology in simple language.\n\nI have a few observations:\n1. The equivalence between equation 2 and equation 3 is mentioned in passing but no explanation is provided. Th equivalence is not clear so I would encourage the authors to provide a short proof.\n2.  Since stiffness depends on the gradients obtained on points in the input space, which in turn depends on the loss, why would a practitioner training a neural network turn to stiffness to diagnose overfitting instead of just looking at the values of the training and validation losses? Indeed, the authors themselves say that a network has overfitted when training and validation losses diverge. The paper fails to motivate why stiffness is better than just looking at losses during training. \n3. The authors mention \u201cThe train-val stiffness is directly related to generalization, as it corresponds to the amount of improvement on the training set transferring to the improvement of the validation set. \u201d. Typically, generalization is evaluated on a held out test set so I fail to understand what the authors mean by this statement. We would expect validation error to underestimate test error  so while they are related, train-val stiffness would not necessarily characterize generalization. It would be interesting to see a train-test stiffness graph to test the authors claim.\n4. The paper fails to motivate the the utility of the concept of \u201cDynamical Critical distance\u201d. Since the primary goal of  paper is to understand generalization, I would like the authors to clarify the motivation to study this quantity. What additional insight does this provide with respect to generalization?\n5. The term \u201cdynamical critical distance\u201d is not used uniformly. For example, it is mentioned as \u201cdynamical critical scale\u201d in section 3.3 and \u201cdynamical critical length\u201d in section 4.2.\n6. While the paper on the whole is written in a clear fashion, I found section 4.4 to be particularly confusing. The authors should consider rewriting that section to make it clearer.\n\nIn summary, the concept of stiffness seems to closely follow training and validation losses and any problem diagnosed using stiffness would therefore be also diagnosed via examining the loss values. This along with other concerns mentioned above mean that I cannot recommend this paper for publication.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}