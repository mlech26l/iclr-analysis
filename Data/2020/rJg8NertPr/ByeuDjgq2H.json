{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper studies the common experimental finding that low level features trained end-to-end in a deep model converge (get \"locked in place\") earlier than higher level features, which may result in problematic undertraining. The focus of the study is not on skip connections, but really on getting adequate training in deeper networks. They posit a \"good classifier hypothesis\" where, once a deep network converged, they fix the top layers (the \"good classifier\") and train only the lower ones. They propose a \"top-down training strategy\" to search where to make the cut for the \"top layers\" of the \"good classifier\", based on the validation set.\n\n\n (+) The experimental results seem encouraging and supporting the author's claim (consistently improve over baseline on WSJ and CHiME-4).\n (-) No WER (not even without a language model) results on WSJ make it harder to (i) compare to other work (is it just that in this case the authors didn't optimize properly in the first place?), (ii) compare the relative gains between with and without the method in WER.\n (-) For an experimental (no theorem) optimization paper, there should be experiments on at least another domain. And in particular one would have expected more analysis of the experimental optimization results.\n (-) (minor) There is no discussion of the link with target propagation or other synthetic gradients.\n\nOverall, I think this could be an interesting paper, but more work is needed to prove the effectiveness of the method, and to analyze experimentally in more details some of the claims from this paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}