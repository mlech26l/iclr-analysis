{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "The paper investigates hyperbolic discounting as a more biologically plausible alternative to exponential discounting in reinforcement learning. First, it formulates a notion of hazard in MDPs as constant exponential discounting and shows that hyperbolic discounting is consistent with uncertainty over the hazard rate. The paper then shows how value functions learned with exponential discounting can be used to approximate value functions with other forms of discounting. Specifically, the paper shows in section 4 how exponentially-discounted value functions can be used to approximate hyperbolically discounted value functions. The paper then presents experiments on a small MDP and Atari 2600 games, showing that learning discounted action values with many different discount rates as an auxiliary task improves performance on most Atari games.\n\nOverall, I very slightly tend towards accepting this paper for publication. While the idea of hyperbolic discounting didn't seem to pay off in terms of performance on the Atari 2600 games, the idea still has some merit as being more biologically plausible than exponential discounting, and may perform better on a more suitable environment. In addition, the discovery that learning many action-value functions with different exponential discounting rates as auxiliary tasks improves performance is quite interesting.\n\nNotes:\n- The footnote on the first page seems to just trail off. It would be better to explicitly state what the takeaway is. I'm guessing humans and animals in general would prefer \\$1M now in the first scenario, but \\$1.1M in the second scenario?\n- The idea of 1/(1-gamma) being an \"effective horizon\" seems questionable. For gamma=0.9 the \"effective horizon\" (aka expected number of timesteps before termination) would be 1/(1-.9)=10 timesteps. However, 0.9^(10)=~0.3486 means ~34% of the probability density is still to the right of that number, so calling 1/(1-gamma) an \"effective horizon\" seems to violate what we normally mean by \"horizon\".\n- A single hazard rate for the entire environment seems counterintuitive. Some states in life are definitely more hazardous than others, which suggests state-dependent discounting might be an interesting idea to explore.\n- Many people in the field think we shouldn't even be doing discounting at all (section 10.4 of Reinforcement Learning by Sutton and Barto), only total reward for episodic problems and average reward for continuing problems.\n- Pathworld experiments seem like they don't show anything useful; the hyperbolic discounting matches the true hazard behaviour of the environment (i.e., the hazard rate is drawn from a distribution) so it performs better than exponential discounting. However, if the environment had a fixed hazard rate then exponential discounting would perform better.\n- Using only 3 random seeds does not seem like enough for the experiments. Also there is no measure of standard error or statistical significance on the graphs.\n- Introducing a hyperparameter that increases computation and memory does not really seem like it's solving the problem, and certainly not \"efficiently\" as claimed in the abstract.\n- How was the value of the new hyperparameter set for the experiments? Were several values tried? More importantly, how can a good value for the new hyperparameter be chosen without prior knowledge of the environment?\n- Atari 2600 games don't really seem like a great environment to showcase the benefits of hyperbolic discounting.\n- It would be interesting to see a comparison with existing auxiliary task approaches, but due to space constraints should probably be left for future work.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}