{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "I am very torn about this paper as the proposed approach is a fairly straightforward extension of past work on the meta-critic approach to meta-learning and the results are pretty good, but nothing amazing. I tend to accept this paper because I like their general direction and think what they are proposing is pretty simple with broad applicability. It should be fairly straightforward to append this idea to most new off policy methods as they come out, so I find their consistent gains across 3 different popular models pretty convincing that this could have value to the community.  \n\nThat being said, the gains are not huge, which does make me think about the potential computational overhead. How much more run time per step does the meta-critic add to the models in the paper? I am a bit worried that the comparisons are not apples to apples from the perspective of the amount of computation/update steps per environment interaction due to the use of the validation data. I wonder if it changes the conclusion at all if the other approaches are given the same amount of computation on their replay buffer between interactions with the environment. For example, more optimization steps on the buffer is another plausible explanation why the meta-critic does better at optimizing the loss. \n\nI also should note that this paper is not the first to propose conducting online meta-learning over a replay buffer. A paper at last year's ICLR [1] did so in the context of lifelong learning, but does not need task labels or tasks and was tested on single non-stationary environments as well. The Meta-Critic approach of this work, of course, still is cool as it does for learning with a Meta-Critic what Meta-Experience Replay does for optimization based meta-learning. However, I thought this should be pointed out as the novelty can be a bit overstated at times. Additionally, the paper would be significantly improved by fleshing out the theoretical motivation for the Meta-Critic approach in more detail. What are the underlying reasons why we would expect it to generically improve single task RL? \n \n[1] \"Learning to Learn without Forgetting by Maximizing Transfer and Minimizing Interference\". Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, and Gerald Tesauro. ICLR-19. \n\nThoughts After Author Feedback:\n\nI really appreciate the response of the authors to my review, which included some interesting new experiments and explanations addressing concerns I raised. I do, however, also see where reviewer 3 is coming from with both major comments. \n\nI don't think that R3A1 is particularly clear and it is an important concern. I think reviewer 3 is saying that Delta(t) in R3A1i will eventually converge to 0 when the policy stops changing while the author argue that it \"need not converge if there are fluctuations at each iteration\". So it not converging seems to be tied to the existence of some source of non-stationarity in the problem. This doesn't seem to be coming from the environment as they are considering single task settings. As a result, I believe the source of the non-stationarity in this case is the fluctuating parameters. Looking at figure 6a it does not seem obvious that the policy has really converged in the traditional sense as its score does seem to be changing to some degree throughout the chart. My best guess is that this is the reason why the meta-loss does not converge. However, I still totally agree that this is a major concern that is very much under addressed. \n\nI also agree that I found the comments about what the meta-critic is doing unconvincing. The authors provided a few different kinds of explanations of what the model could potentially be doing, but this approach to the answer really highlights  how the theoretical benefits of this approach remain unclear. I think it should be possible to directly verify some of these theories with well designed experiments. It feels like a better explanation is necessary in light of the often small margin of difference with baselines before publication. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}