{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The authors introduce a novel decision point discovery method, wherein the VIC objective is constrained to minimize the amount of information between the option and the actions taken along the trajectory. After relaxing the constraint and introducing an upper bound to I(a; o), a tractable algorithm is produced. An implementation is then tested empirically on several partially observed grid worlds and a simple continuous control task on both qualitative bottleneck identification and quantitative benefits as an exploration bonus in a transfer learning setup.\n\nOverall I think the approach is well motivated and interesting, but the resulting implementation takes too many unmotivated modifications to make work, and the results aren't terribly convincing despite this; as such I currently vote for it's rejection. Specifically, the usage of privileged information (x,y coordinates in what is described as a partially observed domain) and the ad hoc choice of which networks had memory (i.e. an LSTM) don't fit the narrative that motivates the work. Constraining the empowerment should be thing that handles spurious diversity, so the need to use x,y coordinates is concerning.\n\nRegarding the empirical results, do all of the baseline make similar use of domain knowledge / privileged information? For example, does your implementation of DIAYN utilize x,y coordinates in the option predictor? Is the Beta=0 case considered? It isn't mentioned, but perhaps it amounts to one of your other baselines?\n\nThe empirical evidence isn't terribly convincing. On two of the three exploration setups, the random network is as performant, and does need a Beta hyper-parameter to tune. Though, to be fair, the connection between decision state identification and a good count-based exploration bonus is loose. The qualitative results are also a bit lacking. I was expecting the doorways to \"pop out\" more; the relatively muddled decision state activations made me wonder if they were really better than DIAYN's.\n\nThis work would really benefit from a quantitative measure of decision state identification accuracy. Some prior work (e.g. \"Grounding Subgoals in Information Transitions\") were able to do this by choosing environments where the quantities of interest were tractable to calculate exactly. This would at least allow us to see if the discovered decision states correspond to those that are optimal under your metric.\n\nRebuttal EDIT:\nThank you for the thoughtful rebuttal. If this were an option, I'd raise my score to a 5. But as my vote is to 'revise and resubmit' (unfortunately translated to 'reject' as per the conference system), I'll leave it in the 'reject' score bucket.\n\nYour rebuttal lessened my concerns about the using (x,y) and only using an LSTM for the policy. I agree these are largely orthogonal issues, and since they were consistent with their baselines, that is fine.\n\nHowever, the response to the unintuitive nature of the \"decision states\" is less convincing. If all you care about is the downstream task performance, why even show the qualitative results or impose the semantics of \"decision states\" on the learned representations? The sandwich bound is novel in and of itself; I understand the need to relate to prior work, but I actually think dropping the language around \"decision states\" (maybe outside of the algorithm's motivation) and talking purely in information theoretic terms would improve the paper.\n\nYour response to [R3] on the setting of Beta hyper-parameter seems to not be supported by the results. You claim that values work well across multiple tasks, but the best reported value for the \"hard\" task (1e-2) is worse than the random baseline on the \"easy\" tasks.\n\nPerhaps only the \"hard\" task matters and the \"easy\" tasks are only of significance due to their usage in InfoBot. But I'd argue that unless your method is dominating existing methods on both without changing hyper-parameters, switching to a more complex (and commonly used) benchmark would be more convincing.  The Atari Suite or the control tasks used in related work (e.g. DIAYN) would be my suggestion.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}