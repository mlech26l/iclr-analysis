{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper considers the task of instruction following where an agent navigates/interacts with a 3D environment conditioned on goals provided in natural language. While several existing approaches use synthetic language for instructions, the authors tackle this problem under the setting of noisy instructions provided by humans in natural language. For this, they use large-scale pre-trained representations (e.g. BERT) as initial parameters for representing the textual instructions. Their main result is the demonstration of transfer from agents trained using synthetic instructions to environments with more variation (e.g. synonyms) or natural instructions provided by humans on two tasks involving object manipulation. \n\nPros:\n1. Nice application of BERT to grounded instruction following tasks\n2. Good empirical results\n\nCons:\n1. Not much technical novelty\n2. Empirical experiments could use a bit more rigor in terms of disentangling the major factors that contribute to performance (e.g typo noise)\n\n\n\nOther comments:\n1. Are the BERT weights frozen or finetuned along with the rest of the model? Does the performance depend on this?\n2. The typo noise (TN) seems to be a key driver of performance. Have you tried adding it to the other baselines like wordPiece Transformer? \n3. What are the scores when training on the test tasks (D.O synonym, natural instructions, etc.) directly? It would be good to establish how well the transfer setup is doing compared to the best RL agent trained directly on the test scenarios.\n\n\u2014\u2014\u2014\u2014\u2014\nPost rebuttal update:\nThanks to the authors for their response and for updating the paper! I especially appreciate the additional experiments, but I\u2019m still confused why the authors do not perform a clear ablation study to support their claims. It seems like the main claimed novelty of the paper is the proposed CMSA method. However, the empirical results are not convincing/rigorous enough to provide the reader information on 1) whether CMSA is a useful method (since it is used only with BERT and does not seem to affect results on its own compared to MP, SA, TN, etc.) and 2) when should one use/not use BERT and CMSA (BERT+CMSA actually does quite poorly acc. to table 5). Further, the other reviewers also pointed out concerns regarding the difficulty of the task and complexity of language used. Hence, I feel the paper still requires some revision to form a coherent story \u2014 updating my score accordingly.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}