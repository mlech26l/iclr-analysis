{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "Thanks for the authors for your detailed reviews.\n\nMy major concerns about the proposed method are whether \"the typicality set\" could be faithfully applied in the small data regime. The authors point me to the interesting Figure 4, which shows that it basically achieves converged performance when $m = 50$ or smaller numbers for some problems. I think this experiment is a strong support for the proposed method. \n\nHowever, I don't agree that the M=1 Gaussian case acts as a strong support for the method. As I said, for some other wired distribution, it is difficult to interpret what the M=1 Typicality set becomes. \n\nThe authors also clarify the difference between different baselines. \n\nOverall, I will increase my score to \"Weak Accept\".\n\n##########################\n\nRecent works have shown that out-of-distribution samples can have higher likelihoods than in-distribution samples for some generative models. To explain this phenomenon and to tackle the problem for OOD detection, this paper adopts \"typical sets\" for identifying in-distribution samples. Specifically, a \"typical set\" is a set of examples whose expected log likelihood approximate the model's entropy. For a Gaussian distribution, the paper finds that a single point typical set locates exactly in the \\sqrt{d} radius, which is usually favored over the high-likelihood origin. Then the paper uses the \"typical set\" to perform OOD for a batch of examples. Empirically they demonstrate competitive performance over MNIST and natural image tasks. \n\nTypical set seems natural for out-of-distribution detection. An important property is that, if one draws a large number of independent samples from the distribution, it is very likely that these samples belong to the typical set (basically Theorem 2.1).  However,  for small n, this property doesn't hold anymore, which leaves here a questionmark whether \"Typical set\" can be used for OOD detection in small n regime. As the author argues, for Gaussian distribution when n=1 the typical locations are those \\sqrt{d} radius points. But this doesn't justify the \"Typical set\". If the distribution is some non-Gaussian wired distribution, the typical locations doesn't seem to make sense at all.\n\nFollowing the previous argument above, the Typical set method requires to perform OOD for a batch of examples. In contrast, the Annulus method can be directly applied to one single test example. \n\nEmpirically, the Typically set doesn't demonstrate obvious advantages compared to the baselines. For both MNIST and natural image tasks, it seems that all methods behave similarly. For comparing such big tables, I would recommend adding a column showing the average ranks among all methods. Beyond that, standard OOD tasks usually evaluate methods using AUROC and AUPR (Hendrycks and Gimpel, 2017). Is it possible to also include such metrics ?\n\nTheorem 2.1 is confusing. It is beneficial to define what P is, and verbally state what the theorem conveys. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}