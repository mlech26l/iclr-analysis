{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "Summary:\nThe paper proposes Generative Teaching Networks, which aims to generate synthetic training data\nfor a given prediction problem. The authors demonstrate its use in an MNIST prediction task\nand a neural architecture search task on Cifar10.\nI do not find the idea compelling nor the empirical idea convincing enough to warrant acceptance at\nICLR.\n\n\nDetailed Comments:\n \nAt a high level, the motivation for data generation in order to improve a given prediction problem \nis not clear. From a statistical perspective, one can only do so well given a certain amount of\ntraining data, and being able to generate new data would suggest that one can do arbitrarily better\nby simply creating more data -- this is not true. \n\nWhile data augmentation techniques have improved accuracy in many cases, they have also relied\nheavily on domain knowledge about the problem, such as mirroring, cropping for images. The proposed\nGTN model does not seem to incorporate such priors and I would be surprised that one can do better\nwith such synthetically generated data. \nIndeed, the proposed approach does not do better than the best performing models on MNIST.\n \nThe authors use GTNs in a NAS problem where they use the accuracy on the generated images as a proxy\nfor the validation accuracy. As figure 4c illustrates there actually does not seem to be much\ncorrelation between the accuracies on the synthetic and real datasets. \nWhile Table 1 indicates that they outperform some baselines, I do not find them compelling. This\ncould simply be because random search is a coarse optimization method (and hence the proposed metric\nmay not do well on more sophisticated search techniques). \n    - On a side note, why is evaluating on the synthetic images cheaper than evaluating on the\n      original images? \n    - What is the rank-correlation metric used? Did you try more standard correlation metrics such\n      as Pearson's coefficient? \n\n\n=================\nPost rebuttal\n\nHaving read the rebuttal, the comments from other reviewers, and the updated manuscript, I am more positive about the paper now. I agree that with reviewer 2 that the proposed approach is interesting and could be a method to speed up NAS in new domains. I have upgraded my score to reflect this.\n\nMy only remaining issue is that the authors should have demonstrated this on new datasets (by running other methods on these datasets) instead of sticking to the same old datasets. However, this is the standard practice in the NAS literature today.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}