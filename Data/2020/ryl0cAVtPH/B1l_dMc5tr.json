{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "\nThis paper conducted an empirical study on why training with warm starting has worse generalization ability than learning from scratch. The paper is interesting, however, it has something unclear to me, as explained below.\n\n\n1)\tThe scale and diversity of the study can be improved. Only three models and three datasets were examined, which might not be representative enough. For example, the popular Transformer model, the large-scale datasets like ImageNet, the language understanding and machine translation tasks, etc. were not included in the study. This may make the study less relevant to many important tasks and domains.\n\n2)\tThe interesting and highlight part of the paper is that it studies many different factors and aspects, including the influence of batch size, learning rate, regularization, moment, denoising, etc. However, I kind of feel that the experimental setting has some fatal problems, which makes the experimental results not convincing. The authors partitioned the dataset into two halves, using the first half for pre-training, and then use the whole datasets for continued training. Although the partitioning is random, given the limited size of the datasets, such a treatment will change the underlying distribution (frequency of the samples during training). The first half of the dataset plays a more important role in training: it was used during pre-training, and also used in training. So somehow the first half was used twice, or at least used more than once,  depending on the numbers of epochs in pre-training and training. This distribution change may make the training a little biased, and at least it is not a fair comparison with learning from scratch (the latter will not have such bias in data). So for a fair comparison, one needs to add some baselines to understand the influence of data frequency change. Without the understanding from this angle, the study may be mis-leading.\n\n\n**I read the author rebuttal, however, I still think the experiments are not comprehensive and the use of data partitions in the experiments are problematic (nothing to do with realistic or not, just for fair comparison with learning from scratch). So I would not change my assessment.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}