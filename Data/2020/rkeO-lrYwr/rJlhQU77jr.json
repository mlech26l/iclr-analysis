{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "\nThis paper empirically examines an interesting relationship between mode connectivity and matching sparse subnetworks (lottery ticket hypothesis). \n\nBy mode connectivity, the paper refers to a specific instance where the final trained SGD solutions are connected by a linear interpolation path without loss in test accuracy. When networks trained with SGD reliably find solutions which can be linearly interpolated without loss in test accuracy despite different data ordering,  the paper refers to these networks as \u2018stable.\u2019 \n\nMatching sparse subnetworks refer to subnetworks within a full dense network that matches the test accuracy of the full network when trained in isolation.  \n\nThe paper introduces a novel improvement on the existing iterative magnitude pruning (IMP) technique that is able to find matching subnetworks even after initialization by rewinding the weights. This allowed the authors to find matching subnetworks for deeper networks and in cases where it could not be done without some intervention in learning schedule. \n\nThe paper then finds a relationship that only when the subnetworks become stable, the subnetworks become matching subnetworks.\n\u2014\u2014\u2014\n\nAlthough finding a connection between two seemingly distinct phenomena is novel and interesting, I would recommend a weak reject for the following two reasons: \n1) The scope of the experiment is limited to a quite specific setting, \n2) there are unsupported strong claims which need to be clarified.\n\u2014\u2014\u2014\n\n1)\nIn the abstract the paper claims that sparse subnetworks are matching subnetworks only when they are stable, but the results are shown in a limited setting only at a very high sparsity. \nThey tested stability on the highest sparsity level at which there was evidence that matching subnetworks existed, but how would the result generalize to other sparsity levels?\nWith lower sparsity level (if weights are pruned less), is stability easier to achieve? \n\nThe paper also focused on cases where matching subnetworks were found by IMP, but matching subnetworks can also be found by other pruning methods. \nAs acknowledged in the limitations section, other relationships may exist between stability and matching subnetworks found by other pruning methods, or in different sparsity levels,\nwhich could be quite different from this paper\u2019s claim.\n\nIn order to address this concern, I think the paper needs to show how the same relationship might generalize to different sparsity levels, \nor alternatively modify the claim (to what it actually shows) and highlight the significance of the connection between matching subnetworks and stability in this highly sparse subnetwork regime.\n\n2) \nAs addressed above, in the Abstract and Introduction, the paper\u2019s claims are very general about mode connectivity and sparsity, claiming in the sparse regime, \u201ca subnetwork is matching if and only if it is stable.\u201d However, the experiments only show it is true in a limited setting, focusing on specific pruning method and at a specific sparsity level.\nFurthermore, the statement is contradicted in Footnote 7: \u201cfor the sparsity levels we studied on VGG (low), the IMP subnetwork is stable but does not quite qualify as matching\u201c\n\nThere are also a few other areas where there are unsupported claims.\n\n\u201cNamely, whenever IMP finds a matching subnetwork, test error does not increase when linearly interpolating between duplicates, meaning the subnetwork is stable.\u201d \n-> Stability was tested only at one specific sparsity level, and it is not obvious it would be stable at all lower sparsity levels where IMP found matching subnetworks.\n\n\u201cThis result extends Nagarajan & Kolter\u2019s observation about linear interpolation beyond MNIST to matching subnetworks found by IMP at initialization on our CIFAR10 networks\u201d \n-> Nagarajan & Kolter\u2019s observation about linear interpolation was on a completely different setup: using same duplicate network but training on disjoint subset of data, whereas in this paper it uses different subnetworks and trains it on full dataset with different data order. \n\nRelated to the first issue, I think some of these stronger claims can be modified to describe what the experiments actually show. \nThe relationship found between stability and matching subnetworks in the high sparsity regime is a valuable insight that I believe should be conveyed correctly in this paper.\n\n\u2014\u2014\u2014\n\nI also have some minor clarification question and suggestions for improvement. \n\nHow was the sparsity level (30%) of Resnet-50 and Inception-v3 chosen in Table 1? (which was later used in Figure 5)\n\n\u2014 In Figure 3 and 5, the y-axis \u201cStability(%)\u201d is unclear and not explained how this is computed. I first thought higher amount of stability(%) was good but it doesn't seem to be true.\n\n\u2014 The ordering of methods for plots could be more consistent. In some figures VGG-19 come first and then Resnet-20 while for others it was the other way around, which was confusing to read. (Also same for Resnet-50 and Inception-v3)\n\n\u2014 There are same lines in multiple graphs, but the labeling is inconsistent, potentially confusing readers:\nFigure 1: (Original Init, Standard) is the same as Figure 4: (Reset), \nand Figure 1: (Random Reinit, Standard) is the same as Figure 4: (Reset, Random Reinit)", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}