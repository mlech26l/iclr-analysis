{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper propose a heuristic algorithm for deciding which random variables to be Gaussianized early in flow-based generative models. The proposed algorithm involves first training a flow without multi-scale training, for example, 32*32*c  - 32*32*c - 32*32*c. Then, it computes the logdet term for each variable at each layer. It then spatially partition the first flow block by two halves of shape 16*16*2c based on max-pooling the logdet term. Then it recursively Gaussianize one half, and partition the other half as 8*8*4c, still using the pre-computed logdet tensors (Ld in the paper). After partitioning, they train a multi-scale model with the learned partition.\n\nWhile I agree adaptive multi-scale architecture is a topic worth researching, and the paper does have some positive experimental results. I think the writing of the paper is very vague and the techniques are not sensible. \n\nWriting: the main algorithm is just depicted in the last paragraph of Page 4. There are not any equations or pseudocode on what exactly does the proposed algorithm do. Figure 1 and 2 are not detailed enough. For example, Figure 2 doesn't explain how to \"Perform splitting based on log-det heuristic and spatial constraints\". I can only guess what the algorithm is. I suggest the authors make the algorithm more clear, and avoid using large paragraphs of natural language to depict the algorithm. \n\nTechnique: \n1. While I agree partitioning based on logdet term makes some sense, I think *recursively* partitioning without updating the logdet terms is problematic. If the flow only have one layer, the proposed algorithm makes sense. However, for a multi-layer flow model. After the first partitioning, the network changes. For example, for a two layer flow 32*32*c - 32*32*c - 32*32*c, the two layers both have 3*3*c*c filters. However, after partitioning the first output layer as two 16*16*2c, the filter of the second layer should have shape 3*3*2c*2c now. It is not clear how to translate the original, single-scale model into a multi-scale one. And the logdet tensor for the second flow layer doesn't mean anything now. \n\n2. For affine coupling layer, we can indeed compute the logdet term dimension wise. However, it is not clear how to do the computation for other types of flows. For example, invertible ResNets, which estimates the log-det term for each ResBlock with an unbiased estimator.\n\n3. I am also not sure with the training algorithm after pretraining. Do we need to remember what pixel to pickup for each max-pooling operation? That has O(s*s*c*L) space complexity. Does the \"gather\" operation baesd on the memorized locations time consuming?\n\n4. Training a single-scale model has higher time complexity than training a multi-scale model. Is this time complexity too high?\n\n=================\n\nUpdate: thank for the authors for their significant effort on revising this paper.\n\nWriting is indeed much better. However there are still many typos (e.g. jabocian, algorithm). Algorithm 1 is better than the original plain-text version, it still doesn't look like even a pseudocode though. I suggest converting the bullets into actual code, e.g., (try to minimize the amount of natural language since it is vague)\n\nFor each layer l \nLd1, Ld2 <- Maxpooling(sth), Minpooling(sth)\n...\nEndFor\n\nI still don't think my concern 1 is addressed. Imagine a single-scale flow\n\ny = AffineCoupling(x)\nz = AffineCoupling(y)\nh = AffineCoupling(z)\n\nvs a multi-scale flow\n\ny = AffineCoupling(x)\ny1, y2 = LCMA-Split(y)\nz = AffineCoupling(y1)\nz1, z2 = LCMA-Split(z)\nh = AffineCoupling(z1)\n\nI don't think |dh/dz1| of model 2 is a submatrix of |dh/dz| of model 1. Because z1 of model 2 is not a part of z of model 1 in the first place. In model 2, z1 is computed with only y1, while in model 1, z1 is computed with the full y.\n\nConcern 3:\nThe authors partially addressed my question. However, the proposed algorithm is still more expensive than a fixed multi-scale architecture, right? A fixed multi-scale architecture such as RealNVP (next scale block is s/2 * s/2 * 2c) is cheaper than a single-scale architecture (next scale block is s/2 * s/2 * 4c). I guess the time complexity of the proposed approach is the letter one instead of the first one. So the improved likelihood still comes with time complexity cost (comparing with a fixed multi-scale architecture).\n\nTo summarize, I can increase my score to 3 as a positive feedback for the author's effort. But I really think this paper still has a long way to go to be complete.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}