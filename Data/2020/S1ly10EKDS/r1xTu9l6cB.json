{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper presents a non-asymptotic analysis of Variance Reduced TD (VRTD), proposed by Korda and La (2015), to apply variance reduction ideas to temporal difference learning, specifically TD(0) with linear function approximation. The algorithm closely follows ideas of stochastic variance reduction (SVRG) which is widely used for large scale empirical risk minimization.\n\nSpecifically, the authors show VRTD converges (in expectation) to a neighborhood of the limit point using a constant step-size. Interestingly, this neighborhood can be made small using a large batch size M in both the IID and Markovian sampling regimes. From a technical standpoint, the work seems novel with interesting results although I am not sure if VRTD offers practical performance gains.\n\nMain points:\n\n1) I am a bit confused by the presence of a constant variance error term in the results. Intuitively, with variance reduction, I was expecting convergence in expectation; not just to a neighborhood with constant error. In other words, the variance error term decaying with $m$ (and not M). This would imply a stronger result than what authors have currently. \n\nNote that vanilla SGD for strongly convex objectives also suffers from a constant variance error term (see for example Chapter 4 in Bottou et al., 2018). However, analysis of SVRG (Johnson and Zhang 2013) shows linear convergence for strongly convex objectives and claims sublinear convergence for convex objectives. While I appreciate the technical challenge in analyzing a \\textit{semi-gradient} method like TD vis-a-vis  analyzing gradient descent for convex objectives, my understanding is that (Bhandari et al., 2018) showed a connection between the two. This connection makes me think if variance reduction can also help \\textit{get rid} of the constant variance error term when analyzing TD(0) with constant step-sizes. Can the authors clarify?\n\nAlso, the authors might find it useful to look at (Lakshminarayanan and Szepesvari, 2018) which does in fact show convergence with a constant (problem instance independent) step-size for the IID case. I think that result only applies with iterate averaging but it might still be useful (certainly as a citation suggestion).\n\n2) As a practical proposal, I wonder if VRTD has a potential benefit over vanilla TD(0). Essentially, the rates showed in this paper require $O(mM)$ samples. I wonder how vanilla TD would perform with $O(mM)$ samples coupled with a simple strategy of reducing the step-size by half when the value-function estimates stop changing. See Chapter 4 in (Bottou et al., 2018) for details. That would probably be a fairer comparison to help convince audience of VRTD as a practical alternative to TD(0).\n\nMinor points:\n- The counter example in Section 3.2 seems out of place. While it is important to point out the errors in previous analysis, I suggest the authors to flesh out the details in an Appendix section. \n- $R_{\\theta}$ and $r_{max}$ seem undefined in the main body of the paper. \n- The constants in Theorem 1 and 2 seem complicated. Is there a way to simplify these for presentation purposes?\n\nResponse to author comments.\n\nI thank the authors for their comments and changes to the draft.\n\n1) The explanation makes a lot of sense. Since we cannot exactly estimate the population gradient, the variance error seems unavoidable with constant stepsize. I like the clarifications in Section 3.2\n\n2) Thanks for clarifying that all the comparisons are done in terms of a fixed number of total gradient computations. Please add this clearly in the paper as well. I think the additional experiments and comparison with the new scheme I suggested above does a good job of convincing the readers of the practical usefulness of variance reduction. \n\nAlthough I am not sure how likely practitioners are to use this (both $\\alpha$ and M are problem dependent), in my opinion the paper presents a novel analysis of an important idea of variance reduction applied to Online TD. The experiments seem reasonable to suggest benefits. I\u2019ll be happy to see this paper accepted. I have also updated my score.\n\nSome minor comments and suggestions:\n\n1) In Theorem 1, constant D_2 depends on $R_{\\theta}$ but there Algorithm 1 has no projection step. I think the authors assume that $\\norm{\\theta^*} \\leq R_{\\theta}$. If so, please state that somewhere. If not, maybe I missed something and would like a clarification. \n\n2) Page 6 should have $e(\\tilde{theta}_m-1)$ instead of $e(\\tilde{theta}_m)$.\n\n3) Before stating the main results in Section 4, please restate the definition of $\\lambda_A$ for better readability. \n\n4) Constant G seems to be undefined for Theorem 2. In fact, it seems that $C_4 = G^2 [1 + \\frac{2\\rho \\kappa}{1-\\rho}]$. This is what I meant by simplifying constants. Ensuring that these constants are optimized for presentation helps.\n\n5) Theorem 2 has a dependence on $d$. Maybe the authors should explicitly mention and discuss that. \n\n6) The result for Markovian case requires the bias term to not dominate. This seems novel to me and would benefit from emphasis in the main body of the paper. \n\n7) Equation 5 should have \u2019m\u2019 instead of \u2019n\u2019.\n\n8) I always find large figures and clear (and) large axis labels very useful. Can the \u2018iteration process\u2019 graphs in Figure 1 be made more reader friendly? I\u2019d  like to see the behavior of vanilla TD more clearly. \n\n9) Section A in the Appendix might benefit from more background. For example, what is $v$. I am thinking this section to be more self contained with the readers not being compelled to read the proof in Korda and La (2015).\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}