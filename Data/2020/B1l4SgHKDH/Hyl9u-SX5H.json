{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The authors make good points, starting from the exposure bias and label bias suffered by the mainstream neural auto-regressive models.\nResidual EBMs are defined and trained using NCE. Experiments on two large language modeling datasets show that residual EBMs yield lower perplexity and generation via importance sampling is of higher quality, compared to locally normalized baselines.\n\nIn generally, the paper is well motivated and interesting. But I have some concerns.\n\n1. Missing important relevant references.\n\nEBMs (a.k.a. un-normalized models, random fields) have been successfully developed in language modeling in recent years. A large body of this paper has been studied in [5,6], including the model and the NCE estimation method. The model Eq.(2) is exactly the model in [5], defining the model in the form of exponential tilting of a reference distribution.\nConnecting and comparing to these previous works are needed.\n\n[1] R. Rosenfeld, S. F. Chen, and X. Zhu, \u201cWhole-sentence exponential language models: a vehicle for linguistic-statistical integration,\u201d Computer Speech & Language,  2001.\n[2] B. Wang, Z. Ou, and Z. Tan, \u201cTrans-dimensional random fields for language modeling,\u201d ACL, 2015.\n[3] B. Wang, Z. Ou, and Z. Tan, \u201cLearning transdimensional random fields with applications to language modeling,\u201d IEEE transactions on pattern analysis and machine intelligence, 2018.\n[4] B. Wang and Z. Ou, \u201cLanguage modeling with neural trans-dimensional random fields,\u201d IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2017.\n[5] B. Wang and Z. Ou, \u201cLearning neural trans-dimensional random field language models with noise-contrastive estimation,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\n[6] B. Wang and Z. Ou, \u201cImproved training of neural trans-dimensional random field language models with dynamic noise-contrastive estimation,\u201d IEEE Spoken Language Technology Workshop (SLT), 2018.\n\n2. I am a little bit concerned that the theoretical contribution seems weak. \nThough Eq. (4) and (5) seem to be novel, I am not sure whether such a contribution is substantial enough to motivate acceptance.\n\nI'm happy to adjust the score if the paper can be better placed in the literature and the authors take efforts to improve the paper.\n\n--------update after reading the response-----------\nBeing well-placed in the literature and properly claiming contribution with respect to prior work is one of the key questions in reviewing a paper. The first version of the paper clearly lacks in this respect. That's the main concern when I gave a 1.\n\nI appreciate the authors' response. The updated paper has been improved to address my main concern, although the added discussions presented in the updated paper is not as clear as the authors' clarifications in the response. I suggest to polish the main text incorporating these clarifications.\n\nGenerally, it is nice to see the successful application of energy-based/random-field-based models in text generation, besides in speech recognition. I update the score to 6 (Weak Accept).\n\nIt would have been better that the following can be further clarified.\n\n\"the partition function estimated via importance sampling would lead to bias favoring the random field language model\" --- this comment is not clear to me. \n\nBoth Eq.4 and Eq.5 give estimates for perplexity. It would be better to clarify different uses of the two equations. If the perplexities are estimated using Eq.4 (as in Table 1), then what is the purpose of developing Eq.5?\n\nHow to calculate the lower and upper bounds of the step-wise perplexity gain at each position in Figure 1?\n\nUnder Figure 1, \"At each position the lower and upper bounds (see Eq. 4) are estimated using 20,000 samples.\" But in the main text, it is said that \"We therefore break down perplexity per position in the generated sequences as in Eq. 5\" at page 8. It is confusing.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}