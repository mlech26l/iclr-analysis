{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #4", "review": "The paper presents a multi-lingual multi-way pseudo-parallel text corpus automatically extracted from Wikipedia.\n\nThe authors use a variety of pre-existing techniques applied at large scale with substantial engineering effort to extract a large number of sentence pairs in 1620 language pairs from 85 languages.\n\nIn the proposed method 1) raw sentences are extracted from a Wikipedia dump, 2) LASER sentence embeddings and language IDs are computed for each sentence, 3) for each language pair candidate sentence pairs are extracted using a FAISS approximate K-nearest neighbor index on the cosine distance between sentence embeddings, 4) sentence similarity scores are computed between the candidate pairs using the \"max margin\" criterion of Artetxe & Schwenk, 2018 and finally 5) sentence pairs are selected according to a language-pair-agnostic threshold on the similarity scores. \n\nThe extraction method is symmetric w.r.t. language directions for each language pair.\n\nStructural metadata of Wikipedia, such as cross-lingual document alignments, is deliberately not exploited (some discussion is provided but I would have preferred an empirical comparison of local vs global extraction). \n\nThe similarity threshold is determined by evaluating training corpora extracted at different thresholds on a machine translation task on De->En, De->Fr, Cs->De and Cs->Fr translation directions, evaluated on WMT newstest2014, and manually selecting the threshold based on BLEU scores. The paper also reports that combining the automatically extracted corpora with Europarl results in strong BLEU improvements over training only on Europarl. BLEU scores on TED test sets obtained using only the automatically extracted corpus are also reported. The corpus has been released.\n\nOverall the methodology presented in the paper is strong and the corpus is likely going to become a valuable tool to build machine translation systems and other multi-lingual applications. However, I am concerned that ICLR 2020 may not be the appropriate venue for this paper, as in my understanding dataset release papers are not explicitly solicited in the Call for Papers https://iclr.cc/Conferences/2020/CallForPapers . The corpus generation method is based on existing techniques, and to the extent that the engineering effort is innovative, it might not necessarily transfer well to data sources other than Wikipedia, thus limiting its broad scientific value. Therefore I suggest to submit the paper to a different venue.\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}