{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper introduces a technique to project n gram statistic vectors into a lower dimensional space in order to improve memory efficiency and lower training time. The paper is motivated by the important problem of trying to improve efficiency of existing language models which can be extremely resource intensive. The authors then compare the performance of n gram statistics with HD vectors on 4 datasets to demonstrate that embedding into HD vectors can preserve performance while reducing resource utilization.\n\n\t1. The main methodological contribution (using HD vectors) are a nice contribution. I would like the authors to clarify why they chose those 3 operations to operate on vectors for this particular task. \n\t2. Lack of a competitive baseline : I would like to see how this method compares with existing techniques to speedup n grams such as Pibiri et al. (https://arxiv.org/pdf/1806.09447.pdf). I believe there are other works which work on speeding up n gram models as well but not comparison is presented.  \n\t3. Minor comment: the authors mention using a SGD classifier but I fail to understand what they mean by this. SGD is an optimization technique and not a classifier so I would like the authors to correct this in the paper.  \n\nAs it currently stands, the lack of strong baselines and the incremental nature of the contribution lead me to believe that this paper does not represent a sufficient advance to warrant publication. I would advise the authors to consider submitting to a more specialized venue (in NLP).\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}