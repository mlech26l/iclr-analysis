{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper studies the phenomenon of incremental learning in several deep models. It starts with analyzing the optimization dynamics of a toy model, and showing that it follows incremental learning, a notion defined clearly in the paper. In particular, it shows that depth affects the strength of incremental learning in the sense that when the depth of the model is increased (especially when going from N=2 to N=3), the maximal initialization value with which incremental learning can occur is increased. In this sense, deeper models experience incremental learning more easily. The paper then moves on to other \u201cdeep linear\u201c models, including matrix sensing, one-hidden-layer quadratic neural networks and diagonal/convolutional linear neural networks, derives ODEs for the evolution of the singular values in the learned models, which is argued to also lead to incremental learning.\n\nI would recommend a \u201cweak accept\u201d for this paper. The nice contributions include a clear definition of incremental learning, results showing the depth\u2019s effect on incremental learning as well as extensions to several other models. My main question is regarding the relevance of the toy model to more realistic models, as I will discuss below, and I\u2019d love to hear more about the authors\u2019 thoughts on this.\n\nBesides being linear, another important simplification of the toy model is that there is no interaction among the hidden units, which is rather crucial for ordinary neural networks. I am curious to what extent the authors think this simplification matters for incremental learning. It\u2019s nice that similar analysis can be extended to other settings including matrix sensing, quadratic NNs and linear diagonal/convolutional NNs. But it seems that there is no theorem analogous to theorems 2 and 3 for those models, and I am curious why.\n\nThe qualitative transition from N=2 to N=3 in the toy model is interesting. Is there a more intuitive explanation for it? Also, from Figure 2, it seems hard to say whether there really is a qualitative change between N=2 and N=3.\n\nSome other suggestions for improvement:\n1. It may be helpful to somehow visualize the bounds obtained in theorems 2 and 3.\n2. Some typos: \n1) \u201cit\u2019s\u201d should be \u201cits\u201d in the last paragraph of section 2.\n2) \u201deffect\u201d should be \u201caffect\u201d in the first paragraph of section 3.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}