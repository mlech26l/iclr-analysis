{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper proposes blockwise adaptivity.  We divide the parameters into blocks, for example in a linear threshold unit the bias term is in a bias term block while the input weights are in an input weight block.  We then average the square norm of the gradients over each block and use the same adaptation based on this average square norm for all parameters in the block.  theoretical and experimental results are given.\n\nThe idea of assigning different learning rates to different types of parameters is old.  Extending this idea to blockwise adaptation is natural and intuitively I would expect this to be an improvement on Adam.  However, I did not find this paper very compelling.  First, I believe that the theoretical results are a straightforward adaptation of know methods.  Second, and more significantly, comparing optimizers empirically is very tricky and I am not convinced that the experiments described here are convincing.  In particular the performance of optimizers is very sensitive to the tuning of hyper-parameters.  I would need to be convinced that the hyper-parameter tuning is sufficient.  Grid search is very inefficient compared to random or quasi-random methods.  Adam has four hyper-parameters and gird search over four parameters is very difficult.  For vision applications of Adam joint tuning of the learning rate and the epsilon parameter is critical --- these parameters are coupled.  It seems extremely likely to me that the move to blockwise adaptation has a profound effect on the optimal value of epsilon.  A thorough investigation of epsilon tuning is needed to demonstrate the value of blockwise adaptation in vision applications.\n\nPostscript:\n\nI still feel that the theoretical analysis provides little to no evidence of in-practice value of the method.  In general I find that theorems guaranteeing getting stuck on a flat plateau to be not very exciting.  What about the exploration goal of local search or mcmc?  In the absence of meaningful theory, the empirical results are what matter.\n\nI have read the response and am not convinced by the comments on optimizing epsilon.  I still believe that empirical claims about optimizers require extraordinary do-diligence in hyper-parameter optimization of both the proposed method and the baselines.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}