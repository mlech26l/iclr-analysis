{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "-------------------- AFTER\nThe original rating of \"Weak Reject\" still holds as the authors failed to provide proper justification for the raise concerns and support their claims through additional experiments.\n\n\"We do not introduce an explanation generation framework, as explainers do. \" - The proposed evaluation requires the explainer of the NLP model to agree with the RCNN in-terms of relevant or irrelevant words, to be considered a good explainer. The RCNN model which is defining the relevant and irrelevant tokens for a prediction task is in fact stating that we can explain the decision of an NLP model in terms of relevant and irrelevant tokens. Hence, the proposed RCNN can also be considered as an explainer. The evaluation task is demonstrating if the other explainers are providing explanations consistent with this new explainer based on RCNN.\n\n\n\"The RCNN is not meant to explain any other models except itself.\" - Unclear\n\n\"Regarding the request for more experiments:\" - The authors don't provide enough justification to \"why they didn't perform more experiments?\"\n\n\" Hence, with our current instantiations, any domain-agnostic explainer can be evaluated\" - The experiment to validate this claim are missing.\n\n\"The novelty of our paper consists in the fact that, to our knowledge, it is the first to (1) shed light over a fundamental difference\" - This is not a technical novelty. This is an exploratory analysis based observation\n\n\"and (2) propose a methodology for evaluating explainers that ...and without human intervention (unlike evaluation type 4).\" -  In Section 5 Qualitative Analysis, the authors are also doing human evaluation like other methods in evaluation type-4 of their related works. Also, doing human evaluation is a strong way to justify an explainer. Though expensive, whenever possible it should be done and is in no way a limitation of current evaluation metrics.\n\nYour model needs labelled data for training RCNN. This adds a constraint on the usability and scalability of your proposed evaluation method. Since RCNN is also black-box, one will required another explainer to explain the RCNN. \n\nIn the worst-case scenario, if RCNN is trained with data such that it considers all relevant words as irrelevant, the evaluation made by RCNN will be incorrect. Hence, \" Success depends on the ability of the RCNN to extract correct subsets of tokens.\"\n\n\n\n-------------------  BEFORE\nThe paper proposed a verification framework to evaluate the performance of different explanatory methods in interpreting a given target model. Specifically, the authors evaluated three explanatory methods namely, LIME, SHAP and L2X for a target model trained to perform sentiment analysis on text data. Authors assume for each input text, there is a subset of tokens that are most relevant and that are completely irrelevant to the final prediction task.  The proposed framework uses a recurrent convolutional neural network (RCNN) to find these subsets. The performance of an explainer is evaluated in terms of overlap between the RCNN most relevant tokens and the most relevant tokens provided by the explainer as an explanation. \n\nMajor\n\u2022\tThe paper lack technical novelty.\n\u2022\tThe proposed architecture uses a RCNN to find the most relevant subset of tokens. Firstly, RCNN is also a black box that provides no intuition behind its selection decision. Secondly, in the absence of the ground truth labels for true relevance and irrelevance of a token in input sentence, this explainer method can also suffer from \u201cassuming a reasonable behavior\u201d assumption. The method assumes that the RCNN is performing reasonably in identifying relevant subsets.\n\u2022\tThe success of the method depends on the ability of the RCNN to extract correct subsets of tokens. The data used for training the RCNN, might have some underlying bias. In that case, the evaluation is not accurate.\n\u2022\tIn related work, for \u201cInterpretable target models\u201d the authors mentioned LIME as an example of explainer functions that explains target models that are \u201cvery simple models may not be representative for the large and intricate neural networks used in practice\u201d. LIME locally explains the decision of a complex function for a given data point using simpler models like linear regression. But LIME itself can be used for generating explanation for prediction of complex neural network like Inception Net. \n\u2022\tThe example used to explain the difference between feature additive and feature selection-based explainer methods, is confusing. Its not clear how in health diagnostics, one will prefer feature-selection perspective. Although the most relevant features used for the instance are important to understand the decision, but in clinical settings sometimes low rank features can also be useful to understand the target model.\n\u2022\tFor text, the relevant features are the individual tokens of the input sentence. Similarly, for images relevance can be important regions of the image. The authors did not have any experiments on images or tabular data.\n\u2022\tIn the experiment section, the comparison is made with only 3 explainer models and for just one task. The experiments are inadequate.\n\u2022\tIn Figure 4, the colormap is not readable.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}