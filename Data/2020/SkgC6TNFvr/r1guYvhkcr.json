{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "# Summary #\nThe paper works on active learning for semantic segmentation, aiming to annotate as few \"blocks/patches\" as possible while training a strong model. The authors proposed to learn a query policy via Q learning, and design states and actions specifically for segmentation. The experimental results show that the learned policy can attend to informative patches and rare classes to learn a model faster and efficiently.\n\n# Strength #\nS1. The paper is well-motivated; the references are quite sufficient.\nS2. The paper clearly states the challenges when applying RL algorithms like Q-learning to image segmentation, which should serve as good guidance for other future work.\n\n#Weakness/comments#\nW1. The writing of the technical part can be strengthened.  The authors deferred the state and action design entirely to the supplementary, while they are the main contributions to the paper.\n\nW2. The proposed algorithms seem to be highly time-consuming. The actions require pairwise comparison, and at every step, the models need to evaluate all the validation images to get the reward.\n\nW3. If I understand correctly, the authors use part of the training data D_T, D_S (of an existing dataset) together with the validation data D_R to learn the policy, and then use the learned policy to select patches from the remaining training data D_V to train the segmentation model. I have two questions.\n1) The labeled data involved in policy training is indeed quite large (validation plus part of the training, D_S + D_T + D_R). Does it mean that to learn a good active learning policy we indeed need a large number of labeled data?\n2) Since (D_S, D_T, and D_R) are used to learn the policy, they should be treated as available training data for segmentation that all the compared algorithms can use without spending the budget. In other words, all the compared algorithms (U, H, B) should use those data to fine-tune a pre-train segmentation network before they start to acquire data from D_V. It would be great if the authors can clarify this.\n\nW4. In applying the policy for selecting patches from D_V, do the authors update the model once on the selected patches, or do the authors train with them for multiple iterations together with other previous selected patches? Since deep neural nets are known to forget what has been learned (i.e., catastrophic forgetting), it's better if the author could clarify this.\n\nW5. The authors include an upper bound in Fig. 4; however, I didn't find the explanation. Why the proposed methods can outperform the upper bound with all the training data, even with only 24% of data?\n\n#Rebuttal#\n\nPlease discuss W1-W5.\n\n- Annotating an entire image is definitely easier to annotators than annotating patches. Could the authors discuss how to design an active learning algorithm by selecting informative images to annotate, and maybe compare to such a method?\n\n- Can the authors discuss Figure 3 more? As H is based on maximum entropy, why is it outperformed by the proposed method? \n\n- There is no explicit mechanism to prevent that the k actions select similar patches. Can the authors provide more discussion?\n\n# Post rebuttal\nThe authors responded to most of my concerns. I'd like the authors to incorporate all their responses into the manuscript or the appendix so that future readers can better understand the concepts and details.  I would like to raise the score to borderline (4 or 5). I modified my scores to weak accept (6) since there is no option in between.\n\nOne concern I still have is W3. 2). Given only 360 images are available, it might be inappropriate to use D_T + D_S (roughly 160 images?) to pre-train baseline models and then use D_R with \"200\" images for validation (early stopping). It will be more appropriate to use, for example, 70% of 360 images for training. This is supported by that many modern datasets use a much larger training set than the validation set. Therefore, I would highly suggest the authors redoing the baseline methods; otherwise, future work that re-splits the data (this is totally valid!) from the 360 images might easily achieve higher accuracy.\n\nThe authors must also reorganize the paper, taking W1 into account. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}