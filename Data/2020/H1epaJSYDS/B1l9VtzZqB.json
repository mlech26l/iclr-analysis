{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #2", "review": "This manuscript proposed to represent the embedding matrix as a small set of anchor embedding and sparse transformation. The paper is trying to be general-purpose, end-to-end trainable, and able to incorporate domain knowledge. Experimental results show that it is possible to compress the embedding in the proposed way without much loss of accuracy.\n \nThe authors propose to find anchor embedding by several methods, such as frequency, clustering, or random sampling. The sparsity on the transform is imposed by L_1. Although I get the basic idea and I am familiar with many of the techniques, it is unclear to me what is the main focus of this paper, and the technical contribution is quite vague. Why is the large embedding matrix a problem? Besides the low-rank form proposed, are there any other ways to compress it? This paper is not well motivated at all. Therefore, I think this manuscript is not ready to publish in its current form.\n\n#####\nThank you for the response! I've increased my score to 3: Weak Reject. Although the idea of compressing the (word) embedding layer using low-rank structures is not new (even with the end-to-end training), the main technical contribution in this paper is to jointly learn the anchor embedding (anchor pre-selected with multiple schemes) and sparse transformation (sparsity achieved via Proximal GD). Moreover, domain knowledge can be incorporated by adding specialized constraints such as orthogonality and selective penalization. \n\nAt first glance, the idea presented in this paper seems not new, and I doubt many people are doing similar stuffs already in practice. I find the explanations on the technical points in Appendix C helpful. The empirical study in this paper looks strong. The authors considered experiments in text classification and language modeling with a number of baselines, which demonstrates the advantages of anchor and joint training in the proposed way. This paper presents several useful heuristics around, but I share the concern with other reviewers about whether the main point is compelling enough, given the existing body of work along with this line. \n\n\n  \n\n\n  ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}