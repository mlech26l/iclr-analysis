{"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The authors study depth-adaptive Transformer models and show that they can perform well even under fairly basic strategies to stop the Transformer early. The paper is well written and the results convincing. There is not a lot of novelty but the results hold well, so it is a clear contribution. One main issue that prevents this reviewer from increasing the rating is the measure of speed the authors use: counting exit at every token. This is a fine measure for inference time, but during training the model proceeds on the whole sequence, right? Could you provide speed numbers for the training step? Is there any improvement over a baseline Transformer or is this technique solely for inference? (Which is still a contribution, but it should be made clear in the paper.)\n\nI thank the authors for the response and clarification. I stand by my score in the light of it.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}