{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #5", "review": "Due to the late rebuttal, I was not able to respond during discussion time.\n\nQ1, Q2) sound and look convincing.\nQ3) I still cannot find details on this in the paper. How the validation set was chosen? What was the size? The authors need to make all the experiments fully reproducible.\nQ4, Q5) ok\n\nMy concerns were sufficiently addressed in the current revision, and I will increase my score. However, the paper still feels close to a borderline, but probably, tending to \"accept\". \n\nAlso, I agree with Review #4, that also wondering about the application of the proposed method to hyperparameter search (Suggestion 1 in my review). Even if this would not be a sota in hyperparameter search, it feels like missing the opportunity to make the paper much stronger, by adding one more nice property to the proposed model.\n\n-----\n\nGenerative models often use a loss function that is a weighted sum of different terms, e.g., data-term and regularizer. Let's denote these weights as \u03bb. The paper proposes a method for learning a single model that approximates the result produced by a generative model for a range of loss-term weights \u03bb. The method uses the following mechanisms i) \u03bb-conditioned layers ii) training with a stochastic loss function, that is induced by a (log-uniform) distribution over \u03bb. The performance of the model is demonstrated on the following problems learning \u03b2-VAE, image compression, and style transfer. The models clearly demonstrate an ability to approximate problem solutions for a range of coefficients. The paper is clearly written. The experiments, however, need future discussion.\n\n1) The beta-VAE experiments (sec. 4.1)\n\nQ1. While models demonstrate a reasonable behavior on Shapes3d dataset. The samples and reconstructions on CIFAR10 (Figure 8) indicate that all models are not trained well. If this is the case, conclusions might be misleading, since the approximating output of undertrained models might be much simple comparing to well-trained ones. Authors may want to provide a comparison of the trained models with conventional VAEs (with \u03b2=1), the reference figures for CIFAR10 are provided, for example, in https://arxiv.org/abs/1606.04934.\n\nQ2. Wider YOTO seems to help a lot, but, what happens to the baseline models of increased size?\n\n\"We select the fixed \u03b2 so that it minimizes the average loss over all \u03b2 values.\"\n\nQ3. Was it done directly on a test set, or were validation-data used?\n\n2) Image compression (sec. 4.3)\n\n\"Finally, a wider model trained with a larger batch size (\u201cYOTO wider batch16\u201d) closely follows the fixed weight models in the high compression regime and outperforms them in the high quality regime.\" (Figure 5)\n\nQ4. How is this compared to the baseline with batch16?\n\nQ5. Authors also may want to provide std for provided metrics. The difference does not look statistically significant. \n\nSuggestion 1: It might also be interesting to see if we can use this technique to perform a hyperparameter search. Train the model, select one the best performing set of hyperparameters, and then train models with this best value.\n\nOverall, the paper proposes an interesting technique, that surprisingly, can work for a range of hyperparameters, and potentially have a high practical impact. However, the empirical evaluation is half-baked, specifically has certain methodological drawbacks e.g., perhaps undertrained beta-VAE model, absence of standard deviations while comparing (close) numerical results, and comparing models with different optimization parameters -- the performance difference might be due to optimization. \n\nI recommend to reject the paper, however, I will appreciate discussions with authors and other reviewers, and will consider changing my score in case of reasonable argumentation.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}