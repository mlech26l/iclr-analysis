{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "#rebuttal responses\n I am pleased by the authors' responses. Thus I change the score to weak accept.\n\n#review\nThis paper presented OPIQ, a model-free algorithm that does not rely on an optimistic initialization\nto ensure efficient exploration. OPIQ augments the Q-values with a new count-based optimism bonus. \nOPIO is ensured with good sample efficiency in the tabular setting. Experimental results show that OPIQ drives\nbetter exploration than DQN variants that utilize a pseudo count-based intrinsic motivation in the randomized chain and the maze environment.\n\nThe new optimism bonus is interesting and convincing with a good theoretical guarantee. The paper would be more clear if the authors add a motivating example in a tabular environment. That is, why does this extra optimism bonus help to predict optimistic estimates for novel state and action pairs. \n\nI appreciate that the authors compare extensive DQN variants using count-based explorations. But the experimental results are somewhat weak, as there are no comparison results on hard Atari games, such as freeway and Montezuma's revenge.\n\nI am willing to improve the score if the authors show better motivation or results on Atari games.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}