{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposed combined form of flexible activation functions with carefully designed principle of choosing activation functions. It shows some gains on stock price and one standard image task over the baseline.\n\nI'm leaning to reject or give borderline for this paper because (1) the paper don't have comparison with neural architecture search. For example, https://arxiv.org/abs/1710.05941 (Searching for Activation Functions). I don't know what the advantage of this approach compared to searched activation. I guess it's less computation heavy and maybe better motivated. But at least the author should give some pros/cons. (2) the paper has two benchmark, stock price prediction and CIFAR-10. I don't understand why as arch paper, it use such non standard benchmark (stock) and non standard arch (LeNet?). I don't think based these benchmark we can make solid conclusion. (3) These model seems introduce quite a bit more hyper-parameters. It's unclear if it is better than tuning other architecture e.g., batch norm/layer norm/dropout or even just optimizer. For example, \"flexible is 0.032\" does this parameter generalize to other dataset? Like if the gain is really significant, like resnet over AlexNet, hyperparam doesn't matter. But if it's marginal win over a weak baseline, the how to get the results is important.\n\nSome comments:\nIn section 2, \" back propagation of these activation parameters by stochastic gradient descent can be\ndone as follows\"\nWhy we need to list the detail backprop formulation here? Are these special? Isn't just autograd?\n\nCan the author explain more for principle 1? What is the \"same domain\" means here?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}