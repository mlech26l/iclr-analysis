{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "The authors study the \u2018non-saturating\u2019 variant of training for GANs and show that it is equivalent to a regular training procedure minimizing a \u201csoftened\u201d reverse KL divergence as opposed to Jensen-Shannon divergence. They show a connection between the two training procedures for more general f-divergence losses. For instance, they show that\n1. non-saturating training on original GAN loss minimizes KL(p/2+q/2 || p) \n2. non-saturating training on KL(p||q) loss minimizes KL(q||p)\n\nThe authors start by arguing about previous analyses of the non-saturating training scheme and present why they do not arrive at the complete picture. Then they go on to introduce f-divergences and f-GAN training before explaining in their notation what precisely non-saturating training means. Then they show that it corresponds to minimization of not the original f-divergence but a hybrid (f,g) divergence. \n\nOverall the paper presents most of its insights as speculative statements and does not do a good enough job of attempting to concretely formalize them.\n\nQuestions:\n1. Why is argument of Nowozin et al wrong when q is parametric?\n2. What are tail weights of an f-divergence?\n3. Unclear why one needs the variational lower bound in f-GAN training.\n4. Why does hybrid training (f,g) converge to the minimization of f? Can the discussion in Appendix F be formalized into a Theorem or a Lemma?\n5. One of the main takeaways of the paper appears to be that initial phases of JS divergence minimization leads to flat gradients which can be problematic - question: is this an artifact of JS being bounded? Could unbounded divergences avoid running into this issue?\n\nThe content organization and highlighting of the main result in the paper can be significantly improved. Since the paper is exposing a theoretical connection as its primary result, I would also recommend a higher level of formalism overall.\n1. Figure 2 is references much earlier than it appears\n2. Tail weights are referenced before they are defined\n3. Formal statement of non-saturating gradient based training captured within a subsection or box. Hard to locate in current draft. Need to read 5 sections to get to it.\n4. Appendix C should be in main body.\n5. Some definitions need to be defined more formally. For instance,\n  a. Hybrid optimization\n  b. Non-saturating training\n\nIt is unclear how significant the contribution of the paper is. It is a clean mathematical observation but the consequences of the connection are not explored and fleshed out. For instance, can by realizing that the non-saturating gradient training is optimizing a different f-divergence can we explain why non-saturating gradient training is more useful? Any insights backed by some simple example settings of synthesized probability distributions would also be useful. The paper also does not propose any new training methods based on the insights uncovered and it is not clear how significant the connection uncovered is with the information presented in the current draft. I believe it is an interesting direction that the paper probes and with a more deeper look into the phenomenon and related directions can be ready for publishing in a venue such as ICLR. In it\u2019s current form I unfortunately don\u2019t think the contributions of the paper are significant enough for acceptance.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}