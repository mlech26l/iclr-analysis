{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "Zero imputation is studied from a different view by investigating its impact on prediction variability and a normalization scheme is proposed to alleviate this variation. This normalization scales the input neural network so that the output would not be affected much. While such simple yet helpful algorithms are plausible there are number of remaining issues:\n1-\tZero imputation, as authors mentioned, is not an acceptable algorithm for imputation and improving on that via the normalization proposed in the paper cannot be counted as an exciting move in this area unless an extensive comparison shows it\u2019s benefits over the many other existing techniques. I am interested to see how would the results be if you compare this simple algorithm with more complicated ones like GAIN or MisGAN. It is argued in the paper that with high dimensional data, your algorithm is more acceptable, but how would it be with in other cases?\n2-\tYour algorithm is only explained with neural net framework, how can we extend it to the other machine learning models?\n3-\tIs batch normalization used in your experiments? Scaling the activation in one layer to reduce its impact on the next layer is somehow similar to what happens in batch normalization, and I am wondering if BN makes any similar effect?\n4-\tPlease provide labels for the x-axes in the figures.\n\n------------------------------------------\nAfter rebuttal:\nThanks for adding the extra experiments.\nLooking at Table 9 in appendix, I am bit surprised to see that sometimes mean imputation works better than MICE (GAIN usually works good with large data). Maybe it attributes to the missing features. How did you choose to apply 20% missingness? randomly?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}