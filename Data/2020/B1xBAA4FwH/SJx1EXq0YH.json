{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #1", "review": "--------- AFTER rebuttal\n\n1) \"We identify issues with current masking procedures as proposed in other papers\"\n\nOne of the major issues with the current masking procedure is that the resulting image is out of the data distribution. Even though your method achieved high accuracy in Table 2 for correctness, the generates images is still out of the data distribution. \n\n2) \"We propose a cost-effective masking technique that doesn\u2019t require retraining of the underlying classifier\"\n\nThe authors compared against zero and gray masking for correctness. None of those masking methods require retraining of the underlying classifier. It is not clear, which previous masking technique required retraining of the underlying classifier?\n\n3) We also further show that when performing a comprehensive evaluation, there is no one clearly better explainer and thus practitioners need to be careful about which explainer they choose.\n\nThis is an observation made upon through exploratory analysis and is not a technical novelty.\n\n4) \" Confidence on the other hand, also informs us about the per-instance behavior.\"\n\nThe confidence measures the change in probability assigned to the ground truth class. Table 3 should also show the variance in the confidence to understand the instance-level behavior.\n\nThe experiments given in the paper, it looks like confidence and correctness are positively correlated. An example of the model where they are not positively correlated will help the reader understand the importance of each of these terms.\n\n5) \" Effect of Thresholding on results\"\nThank you for the explanation and new experimental results.\n\n\n\n\n------------------------- BEFORE rebuttal\nThe paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. To evaluate correctness, the authors proposed to study the change in the classification accuracy of the target model, under a perturbed dataset where the most relevant regions (as given by explainer) of the image is preserved and the remaining content is replaced with non-informative backgrounds for the target class. For consistency evaluation, the authors proposed to apply transformations like rotation, translation and flip that doesn\u2019t semantically change the input image. For confidence evaluation, they compared the prediction performance on the original image, masked image (only salient regions) and inverted masked image (only non-salient regions). \n\nMajor\n\u2022\tThe paper lack technical novelty.\n\u2022\tThe confidence component looks redundant and can be incorporated in the correctness component.\n\u2022\tThe inverse saliency map idea is already proposed in \u201cEvaluating the visualization of what a deep neural network has learned\u201d for evaluating saliency maps. There the authors gradually replace the most salient regions with random noise and observe a decrease in prediction accuracy.\n\u2022\tMost of the saliency maps producing methods, generate continuous maps. For making, we need to convert the continuous map to binary by using a threshold. An analysis of choosing different values as threshold is missing. By choosing an appropriate threshold, the size of the most salient region can be controlled. Thus, although Grad-Cam spread saliency over large area, we can use a higher threshold to define the binary mask.\n\u2022\tGrad-CAM, integrated grad and smooth grad are all gradient-based saliency maps. There are perturbation-based saliency maps, which aims to find most salient regions such that removing those regions produce a maximum drop in prediction accuracy. Example \u201cInterpretable Explanations of Black Boxes by Meaningful Perturbation.\u201d, \u201cObject detectors emerge in deep scene cnns\u201d .  An evaluation of such methods is missing.\n\nMinor:\n\u2022\tThe text in the figures has very small font size and is not readable.\n\u2022\t\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}