{"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #5", "review": "Note: I was asked to write a last-minute review for this paper since the overall rating of the other reviews are not consistent. Therefore, the review is rather brief and I will comment also on concerns raised by the other reviewers.\n\nThe paper introduces a new MAML algorithm based on evolutionary strategies (ES) for reinforcement learning tasks. Compared to prior MAML algorithms requiring an estimation of the Hessian, ES-MAML demonstrated to be more stable and efficient. Overall, the paper is well motivated, well written and uses a sound mathematical formulation of the solution approach. Furthermore, the results are convincing and show quite some promise.\n\nConcerning the remarks from Reviewer #3, I believe that it is totally fair to use here a simple ES algorithm that still shows reasonable performance. Of course, we would expect that other ES algorithms might perform better, but this is clearly not the point of the paper. Furthermore, also other papers [1,2] showed that very simple ES algorithm can perform very well on weight optimization of policies. \n(Remark: since there is no page limit for refs, I would recommend to cite [1,2] in the paper)\n\nI share some concerns from Reviewer #4 regarding the hyperparameters. By now, it is well known that hyperparameter tuning can improve the performance of RL algorithm quite a bit and is sometimes even the main factor for superior performance. The authors wrote in their reply to Reviewer #4: \u201cIn fact, we did not perform much tuning,\u201d. I would like to reply: In fact, this is not a very useful answer.  If there was hyperparameter tuning involved, the amount has to be quantified (in the appendix) and the same amount should be applied to all approaches being compared in the paper.\n\nFurthermore, I missed a discussion about the limitations of the approach. For example, I would expect that the approach will fail if the networks get too large (and thus  the parameter space is too large (>1Mio Parameters?)) and the task is fairly complicated such that the parameter space is not too redundant. I think there is a reason why people tried to use ES for optimizing DNNs for decades, but failed, and now nearly everyone uses GD variants. So, the authors should be more explicit about potential failure cases and limitations.\n\nSmall remark: I haven\u2019t found a description of the architectures used in Section 4.4. Since the paper should be self-contained, I would recommend to briefly make this explicit in the appendix.\n\n[1] Patryk Chrabaszcz, Ilya Loshchilov, Frank Hutter: Back to Basics: Benchmarking Canonical Evolution Strategies for Playing Atari. IJCAI 2018: 1419-1426\n[2] Lior Fuks, Noor Awad, Frank Hutter, Marius Lindauer:\nAn Evolution Strategy with Progressive Episode Lengths for Playing Games. IJCAI 2019: 1234-1240", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}