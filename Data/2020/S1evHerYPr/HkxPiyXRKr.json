{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #3", "review": "The paper proposes to meta learn the objective function of a policy gradient algorithm using second order gradients of the objective function w.r.t the state-action value Q. \n\nThis is an interesting approach, however, I think the experimental evidence is not sufficiently convincing. \n\n- In particular, I think the most important baseline that is compared against is not RL2, but DDPG: RL2 is not designed to generalize but to learn quickly on new tasks from the training-task distribution. Because the proposed algorithm does not depend on the observed states, it generalizes much better, but is also much slower than RL2. On the other hand, it shares a lot of design choices with DDPG: Using TD3 and Double Q-learning, as well as using as objective function the Q-values. \nLooking at Figures 2, it is not clear that the proposed algorithm is substantially better than DDPG. \n- Cheetah, Hopper and Lunar Lander are very simple environments. Evaluation on (slightly) larger scale environments would show that the algorithm can scale. \n- The authors claim that the algorithm allows sharing of exploration strategies, which I don't believe can be the case based on it's current design.\n- Lastly, I have a question about Fig 5a vs. Fig 2b: Shouldn't the performance of MetaGenRL be the same in both? It appears to perfom much better in Figure 2b.\n\nMinor remark/question (didn't influence score):\nThe authors claim in the very first paragraph (and in the 4th paragraph) that inductive biases in humans are learned by natural evoluation through \"distilling the collective learning experiences of many learners\" by \"learning from learning experiences\". I'm not familiar with the relevant literature, but this seems like a strong statement which I believe should be supported by a citation. \n\nEdit because I can't make my response visible to authors anymore:\nThank you for your response to my review and apologies for my delayed answer.\n\nAfter reading your responses I agree that PPO is a fairer comparison than DDPG and that you are outperforming PPO is promising.\nI further agree that it is relevant to show that RL2 overfits (although I personally don't find that very surprising - see below). \n\nHowever, I still don't think that RL2 is a relevant baseline for this approach. There's a fundamental trade-off between the speed of adaptation and the amount of overfitting. \nIf I want to adapt very quickly (like RL2 does), I need to leverage as much task-information as possible, thereby overfitting to the task-distribution. \nOn the other hand, MetaGenRL is slow, it has training speed comparable with gradient-based approaches (by generalizes better by construction because it e.g. doesn't receive states as inputs). \nConsequently, because MetaGenRL doesn't offer any speed improvements over gradient-based approaches, it should be compared to them, and not RL2.\n\nTaken both the positive results vs. PPO and the negative results vs. DDPG, together with the fact that there's no learning speed advantage for MetaGenRL, I would see it as an interesting, and promising, research direction, but so far without proof that it can advance state of the art, as it looses to RL2 in terms of speed and DDPG in terms of final performance.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}