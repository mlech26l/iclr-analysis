{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper studies approximation of the potential energy of molecules by a message passing architecture. The work builds upon [Gilmer et al., 2017] and the contributions are two-fold:\n1) The creation of new datasets to learn and test such architectures on and the augmentation of an existing dataset in order to account for energies at non-equilibrium states.\n2) A proposed modification to the MPNN architecture proposed in [Gilmer et al., 2017], in order to account for physical properties in the message-passing procedure.\nThe performances of the architectures are studied with numerous numerical experiments.\nThe paper is overall well-written and clear.\n\nThe new dataset utility is sound and well-motivated. Unfortunately I can not further motivate upon this, as I am not familiar with this area.\n\nFrom the point of view of the proposed architecture, the work is quite incremental. The bond type information, previously included as feature, is now transferred to an architectural modification. On the other hand, many different modifications (although no substantially different from each other) are proposed and tested (although no results about the different modifications are reported - it could be nice to have them in an appendix). This motivates the \u2018weakly accepted\u2019.\n\nThe authors also considered the idea of adding additional learning modules (and a related loss) to help the model learn more 'physics interpretable' hidden states. While it does not seem to give notable gains here, it is an interesting idea and I believe deserves further experimentations in the future.\n\nThe experiments are numerous and various, and they offer a very good overview on the goodness of the model (and its limitations). They first compare with the baseline on the augmented dataset, and they show notable gains on the MPNN baseline. The ability of the network to reproduce the energy curve at different interatomic distances is then studied on the different dataset and in different settings, showing gains over the baseline. The authors also report some negative results and experimental interpretation of the model hidden states, which are also an important contribution in my opinion. \n\nFurther comments:\n\n1. No details are given about the training of the models. I think a small paragraph (or larger and reported in the appendix) should be added.\n\n2. Even if it builds upon previous work, the (VI)MPNN model may be further explained. For example, what type of functions are M_t and R? The explanation on the considered modifications of MPNN may be clearer (maybe with the introduction of a more mathematical notation). \n\n3. How long is the message diffusion (T)? What is the effect of larger / smaller T\u2019s?\n\n4. What\u2019s the point of equations (5) (6) (7)? They are exactly the same and they do not add any information. It would be more useful to explain what type of function R is in my opinion.\n\n5. Table 1, Auxiliary estimates: Are these the results obtained by the model a.ii trained to jointly learn the energy and the properties i) ii) iii) ? In what sense they improve on the baseline? This part was not clear to me.\n\n6. Section 5.2: \u2018[\u2026] we combine our proposed physics integration strategies, namely bond type\nspecialised\nnode updates (case a.ii) of Section 4.2) and auxiliary estimations of physical properties,\ninto the VIMPNN model [\u2026]\u2019. I do not understand this sentence. Isn\u2019t in fact the VIMPNN architecture the same as MPNN with the modification a.ii? In what sense do you integrate a.ii in it?\n\n7. If I understood correctly, the main final objective is to be able to characterize the minima of the energy. In this case, could you asses the performances of the two methods (MPNN) and (VIMPNN) by measuring some kind of distance from the approximated minima from the actual one?\n\nTypos:\n\nSection 5.3 first line: \u2018We investigate the VIMPNN\u2019s the ability \u2026\u2019 -> \u2018We investigate the VIMPNN\u2019s ability\u2019\n\nSection 5.2 Fourth sentence: \u2018The first seeks to demonstrates a the model\u2019s \u2026\u2019 -> \u2018The first seeks to demonstrates the model\u2019s\u2026\u2019", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}