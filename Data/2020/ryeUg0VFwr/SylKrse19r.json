{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "#rebuttal response\nThanks for your response and the additional experiments. I do think it is a very interesting paper but I keep my score. First, the improvement of their model is not very significant both in Table 1 and the standard RL setting. Second, I am still not clear about how off-policy learning benefits the study on standard RL. Since exploration and exploitation interact with each other, disentanglement between them cannot reflect the original difficulty of RL problems. Different exploration strategies or balancing methods between exploration and exploitation will produce different data. I guess this is why the data generation process is a common concern raised by both Reviewers 1 and 2. I think the response of the authors is not enough to clarify this. In addition, I cannot find Figures 4a and 4b. I think this paper is not ready for publication right now but I encourage the authors to improve this work and resubmit it in the future.\n\n\n#review\n\nThis paper collects a replay dataset of Atari to propose a benchmark for offline (batch) reinforcement learning. Experiments on this dataset show that offline RL algorithms can outperform online DQN. Then, this paper presents a new off-policy RL algorithm, which uses random convex combinations of multiple networks as the Q-value estimator. The experimental results show that it outperforms distributional RL algorithms (i.e., C51 and QR-DQN) in the offline setting and performs comparably in the online setting. \n\nThe idea of randomly combining Q networks is interesting and paper is well written to demonstrate their key ideas. However, I have some concerns.\n\nWhy are off-line RL algorithms necessary? I am not sure this paper is relevant to the community of standard RL. This paper does not show the usage of their method to a standard RL agent. I am happy to change my score if the authors can clarify the motivation of studying the offline performance of conventional RL algorithms and clearly show the improvement over baseline algorithms in a standard RL setting rather than only show insights for potential usage in online RL.\n\nIn my opinion, the results are not significant to support their claims. Such a small gap (4.9% as shown in Table 1) over baseline models may result from considerable hyper-parameter tunning. In addition, the figures and tables in this paper do not show deviations and confidence intervals.\n\nSince REM has similar architectures with attention networks, it will be better to include an attention-based Q network as a baseline model.\n\nIn the right sub-table of Table 1, I wonder why the performance of online QR-DQN in this paper is much lower than that reported in the original papers (media is about 200%).\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}