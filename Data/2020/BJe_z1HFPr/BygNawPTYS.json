{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes Resizable Neural Networks, which trains networks with different resolution scalings at the same time with shared weights. It serves as data-augmentation and improves accuracy over base networks. Additionally, the same technique can perform an architecture search. Experimental results show significant accuracy gains.\n\nThe reported accuracy gains are substantial. The proposed method is potentially useful in many applications. However, several details are missing or hard to understand. Without additional descriptions, it is not straightforward to implement the method. Thus, I suggest for rejection. The score might be raised depending on updates and code release.\n\nMajor comments:\n1) Algorithm 1 has \"predefined spatial list L.\" How to choose it in practice?\n2) Algorithm 1 indicates that the training time is len(L) times longer. Additionally, according to the implementation details, Resizable Networks are trained two times many epochs. It seems hard to justify such a longer training time.\n3) This is similar to (2), but why Resizable-NAS is better than the all len(L) models separately to find better architectures?\n4) In Sec. 3.4, how the \"target model\" is selected after the training of Resizable Networks?\n5) Why is Resizable-Adapt omitted from Table 2?\n6) References are out-dated. For example, there are no \"Figure 5.\"\n\nMinor comments:\n1) Are any ablation studies available for Fair Sampling?\n\n===== Update\n\nThank you for the response and update. The revision made the paper easier to understand. However, I still have concerns about the presentation of the paper, and I keep my score. I think the novelty and experimental results are significant and match the bar of ICLR. If the other two reviewers think that the revised paper is fairly well-written and recommend acceptance, I will not challenge the decision.\n\nMajor comments:\n1) I do not understand how the training times of random-sampling in Appendix C were estimated.\n2) Concerning Table 11, how was the result when we randomly select each scale and train network with the same number of epochs with fair-sampling? If there were no much difference with the result in Table 11, it seems fair sampling does not have clear advantages over random sampling. If so, I suggest to alter fiar-sampling by random-sampling and reduce the complexity of the proposed method.\n\nMinor comments:\n1) Table 10 has two captions.\n2) I did not understand that L is a list of possible scaling factors (scalar) in the initial review. I guess it is partially because I did not understand why the number of sampling should be len(L) for each mini-batch (it is actually due to fair-sampling). I think adding some remarks on it will help to make the pseudo-code easier to understand. (nit: for j= 0, ..., len(L) should be for j=0, ..., len(L) -1 or for j=1,  ..., len(L))", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}