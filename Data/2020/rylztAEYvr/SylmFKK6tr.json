{"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "(post rebuttal) I appreciate the authors for detailed rebuttal. I stay with the original score based on the following reason.\n\nIn RAML you sample from exponentiated reward distribution and do maximum likelihood (minimizing forward KL in classic control as inference framework). How you sample depends on the problem assumption. In the original paper, they used a heuristic based on edit distance from ground truth, but in theirs they did not have assumption that you can evaluate correctness (as used in external filter). In this paper's context, the sampling naturally comes down to rejection sampling. Therefore I keep the original stance regarding the similarity between this work and RAML. \n\n---\n\nThe paper proposes an iterative data augmentation approach based self-generation and filtering with success criteria. The authors justify the algorithm as an EM procedure of maximizing \\log p^*(y|x), where p^*(y|x) \\propto p(y|x) * p(c=1|x,y). They demonstrate that the iterative data augmentation procedure can provide significant gains to SOA models in molecule generation and outperform MLE+RL method in program synthesis datasets.\n\nStrengths:\n- A simple approach that can be added to any seq2seq translation where success metric can be evaluated efficiently\n- Demonstrated results on multiple applications\n\nWeaknesses:\n- The method requires being able to evaluate constraints  \n- The approach is simple and does not seem to present significant novelty over prior methods. Particularly, the approach could be considered as nesting RAML [1] updates with (1) low temperature, (2) self-generated trajectories. \n\nOther comments:\n- Some missing references [1, 2]. [2] also studies molecule generation and proposes approaches similar to RL + MLE. \n\n[1] Norouzi, Mohammad, et al. \"Reward augmented maximum likelihood for neural structured prediction.\" Advances In Neural Information Processing Systems. 2016.\n[2] Jaques, Natasha, et al. \"Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}