{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #7", "review": "This paper proposes a categorization of inner layer weights to be linearly or non-linearly correlated with the output. The motivation on why this is important is somewhat weak in the paper. But I could see cases where this is important, if there is supporting evidence that it helps with interpretability. However, I did not see that in this draft unfortunately. \n\n\u201cGLMs \u2026 interactions of non-linear activations are not involved\u201d \u2013 the link function in all GLMs except linear regression is non-linear. So I am not sure what this statement means. \n\nThe definitions of \\mathbb{L_k} and \\mathbb{H_k} are not clear when they are introduced. The example given talks about l_1 and h_1, not about \\mathbb{L_k} and \\mathbb{H_k}.\n\nI didn\u2019t get the point of the function B(.). Why not directly use z \\in {0,1} ?  Similarly, I did not get the point of introducing g(.), why not just push the entire mapping into \\phi ? The notation is made unnecessarily complicated. \n\nThe sentences after eq 7 are unclear to me. What does \\epsilon \\sim p(\\epsilon) even mean ? Is \\epsilon the parameter or the random variable here ? What is \\epsilon used for ? What is m(.) ?\n\nThe toy example is simple. But it does not address why such a classification into h_k and l_k is helpful and how it can be used. Can the authors motivate the usecases where such an interpretation (using the toy example) is useful ?\n\nThe experiments are not convincing. The MNIST experiment regarding identification of contours is very vague. There are tons of other methods that give better interpretation. The identification of \u201clong\u201d feature to be linearly correlated with the output can be done in a much faster and easier way, by simply checking individual feature correlations. The strength of such a method that the authors are proposing would be to extract useful information for cases where the input features are /not/ linearly correlated, while there are some inner layer features which /are/ linearly correlated. This could help in understanding the landscape of the classification better. But I did not see that happening here. Happy to be proved wrong by the authors or other reviewers if I am missing something here. Similarly, there is lot of work on sparsification/pruning of NNs, in light of which I am not sure what Section 6 adds.  \n\nAppendix A.4 is redundant, softmax/sigmoid being a glm is well-known. Also, in the main text (e.g. last para page 1), the reference is to Appendix A.3, while it should be to A.4. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}