{"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes a new way of training variational dropout which is adaptive to input samples due to the proposed sparsity-inducing beta-Bernoulli prior. The authors provide a good motivation for their model, introduce beta-Bernoulli and dependant beta-Bernoulli prior and propose the method in the variational inference framework. \n\nConcerns:\n1) The main concern relates to the significance of benefits from the input dependency property. From the Tables 1, 2 we can see that BB is comparable with DBB and the latter is not uniformly better than the former in terms of error, xFLOPs and memory. This similarity in the performance is more significant for LeNet5-Caffe network, for CIFAR-10 and CIFAR-100 datasets. Is the overhead of DBB worth the benefits it gives? \n2) The second question is about the memory consumption of DBB. The authors use two stage pruning scheme for DBB: at first, they prune DBB using the beta-Bernoulli dropout, then prune the network for each input individually. It means that DBB should keep all weights after the first stage, in other words the memory consumption of DBB is the same as BB. Some clarifications about this concern are necessary. \n\nOverall, the paper proposes interesting and well-motivated method for training sparse networks. Although, there are concerns about the DBB extension I would recommend considering this paper for acceptance. \n\n---------------------------------------\nUpdate after author rebuttal\n\nThank you for your thoughtful response. You addressed my second concern about memory consumption of DBB. Indeed, the run-time memory mostly consists of activation maps, therefore DBB can benefit from input-dependent sparsity. Considering the first concern I still think that the advantage of DBB is not clear and I agree with AnonReviewer3 who said that except LeNet-500-300 other results are mixed. \n\nHowever, overall I do think that the proposed method is novel, well theoretically grounded and is proved that it works comparably if not better than the state-of-the-art approaches. Therefore, I remain my score as weak accept. \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}