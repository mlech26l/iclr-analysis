{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper presents a model with beta-bernoulli dropout of neurons for network pruning. The model assumes the probability to dropout a neuron is a function pi(phi, x), which depends both the beta variable phi and the input x. The model is trained with stochastic gradient variational Bayes with continous relaxation. The model and algorithm sound, and the intuition of determining the dropout probability based on the importance of each dimension makes sense. \n\nOne weakness of this work is the lack of large-scale experiments, for example, pruning a MobileNet on ImageNet. This work also seems incremental due to its resemblance with CD.\n\nFigure 1: bottom-right figure (input-regions pruned by DBB) is missing?\n\n====\nUpdate:\nThe authors do address my concern #2. After reading other reviews and reading the revised paper I do think this approach of this paper is novel and can potentially lead to a gain. However I still don't think the experiments are convincing enough. It needs to be tested on a larger variety of models (ResNet or MobileNet) / datasets (ImageNet, etc.) / tasks (vision, nlp, etc.) to prove its significance. Therefore I won't change my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}