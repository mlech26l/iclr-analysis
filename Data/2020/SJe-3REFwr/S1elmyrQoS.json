{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #4", "review": "The paper describes a variation of the Transformer sequence2sequence architecture for neural machine translation.\n\nThe proposed innovations are:\n1) moving the token-wise feed-forward blocks of the transformer to be in parallel with the self-attention blocks rather than interleaved between them as in the original architecture.\n\n2) combining in parallel the self-attention and FFN blocks with dynamic convolution blocks.\n\nThe methods are evaluated on WMT14 En->Fr, IWSLT2014 De->En and IWSLT2015 En->Vi.\n\nFor 1) a small improvement of 0.5 BLEU over the original Transformer is reported on a single task, with no significance analysis. Given that the improvement is small and results from a single experiment, it's not possible to draw strong conclusions from it.\n\nFor 2) the model shows strong improvements over the original Tansformer, but it's on pair with the Dynamic convolution Transformer of Wu et al. 2019 from which this work is based, despite having more parameters.\n\nOverall it's not clear from the reported evidence that the methods proposed in this paper represent an improvement over Wu et al. 2019.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}