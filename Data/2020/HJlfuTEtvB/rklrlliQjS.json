{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "Summary:\n\nThis paper introduces a novel way to find loop invariants for a\nprogram. The loop invariants are expressed as SMT formulas. A\ncontinuous relaxation of an SMT solver is introduces mapping\nevery SMT formula onto a truth value \\in [0, 1]. This relaxation\ncalled a continuous semantic mapping is decided such that every\ntrue formula has a higher value than every false formula. This\nallows an invariant to be learned.\n\nNovelty and Significance:\n\nThis work is interesting and although authors do seem to be\noutside of the community, I firmly believe is appropriate for\nICLR. If the claims made by the paper are true they constitute\na significant contribution to the field of program synthesis\nand program analysis.\n\nTechnical Quality:\n\nThe evaluation was fairly thorough, but the paper can be\nstrengthened massively with a few small changes and additions.\nIt might be more helpful if there was a sense of how many problems\nnone of the systems can do and how complicated of a program can\nthis system extract a loop invariant from. Why were was it these\nparticular 12 that work? Are there examples that don't?\n\nI don't know why all this time is spent on t-norms when behavior\non them is fairly similar and the simplest norm works best.\n\nLots of details are missing in this paper. How much training data\nwas generated? How long did it take? Does training data need to\nbe generated for each example? If so is that included in the\nruntime for Figure 3?\n\nThe paper talks about neural architecture, but all I see is\neffectively a curve-fitting task for some template. This feels\ndifferent from the code2inv paper where a program can be fed\ninto the system and the pretrained model emits the invariant.\n\nClarity:\n\nNot enough of this paper concentrates on the novel aspects of the\napproach. Section 5 discusses template generation, but not in\nenough detail that I would be able to replicate this work. I\ncould also not find enough details in the Appendix.\n\nI don't know why vital page space is spent on defining completeness\nand soundness.\n\n\nPossibly related work as relaxations to SAT/SMT solvers do exist in the literature.\n\nGuiding High-Performance SAT Solvers with Unsat-Core Predictions\nhttps://arxiv.org/abs/1903.04671\n\nLearning to Solve SMT Formulas\nhttps://papers.nips.cc/paper/8233-learning-to-solve-smt-formulas.pdf\n\nNotes:\n\nYou cite Si et al. for LoopInvGen when you should be citing Pathi\nand Millstein in the third paragraph on page 2.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}