{"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper gives theoretical and empirical results for a gradient clipping variant of Adam they call ACClip.  While the theoretical analysis is rather  sophisticated and nontrivial, I personally do not believe that analyses of this form are of any value in guiding practice.  But that is a long discussion that is not specific to this paper.  The bottom line is that for me it is mainly the experimental results that matter.\n\nThe experimental results are not compelling.  It is now clear that careful hyperparameter search is critical to drawing experimental conclusions about optimizers.  This paper simply states the hyperparameters used with no discussion of hyperparameter search. I strongly believe that any claim about optimizers needs to be backed up by experiments with very careful hyper-parameter optimization.\n\nPostscript:  I have modified this review in response to the authors.  I remain unconvinced that the theory is providing anything more than an intuitive hypothesis that Adam is importance when the variance is large.  Since Adam and RMSprop are explicitly damping variance in the gradients, this intuition is reasonable even before we prove any theorems.  I still believe the theorems do not add really add anything to the intuition and it is the experiments that matter.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}