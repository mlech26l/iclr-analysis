{"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper explores the use of replicating neurons across and within layers to compress fully connected neural networks. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes. \n\nStrengths: a lot of nice experiments with clearly advantageous results are given.\n\nWeaknesses: One obvious baseline missing is sparse compression, which can be achieved using either l1 regularization, or hard thresholding + fine tuning, both of which are easy to implement and appear in several works, e.g.\n\nScalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization (Yang, Ruozzi, Gogate)\nTraining skinny deep neural networks with iterative hard thresholding methods (Yin, Yuan, Feng, Yan)\n\n... many others just via googling ... \n\nAlso, I think this work should be compared with compression schemes that work via kronecker product, which seem very similar to this scheme (but where the kronecker matrix is binary to produce replication)\n\nCompression of Fully-Connected Layer in Neural Network by Kronecker Product (Zhou, Wu)\n(more via google)\n\nOne obvious advantage of replication over kronecker product is lower complexity, but nonetheless, the methods belong in a similar family.\n\nOtherwise, I think the work makes sense, the idea is nice, and the results show promise!\n\nAfter rebuttal: I have read the rebuttal and the authors have basically addressed all my concerns. It is a bit disappointing that simple L1 regularization can give competitive results, but the fact that the authors are willing to do the experiment and incorporate the results convinces me that there's nothing being hidden here, and the reader can make a fair and informed conclusion, so I have no more complaints.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}