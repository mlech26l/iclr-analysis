{"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "Summary\nThis paper proposes a network quantization method. Different from previous methods focusing on network-level or layer-lever quantization, this work pays attention to kernel-level quantization. Specifically, they use a hierarchical reinforcement learning framework to search in the search space related with quantization. The experiment result validates the significance of the work.\n\nStrength\nThe paper provides us with a new insight into network quantization. Even though the extension from layer-level to kernel-level is straightforward, the improvement is significant and meaningful. The experiment result demonstrates its efficiency in real applications.\n\nWeakness\n1. The algorithm of the paper is similar with previous work HAQ, which is based on DRL to guide the search procedure. Thus, the novelty of algorithm is somewhat weak.\n \n2. For kernel-level quantization, this paper proposes a hierarchical DRL method. However, I didn't see the importance of the hierarchy. The author may discusses more about this and compare it with flatten algorithms for ablation study. \n\n3. The paper didn't compare their methods with other baselines on the same level. I think some algorithms can be applied into kernel-level quantization directly. Based on the same level comparison, the efficiency of your method can be seen more directly. \n\n4. The description was not written well. For example, the detail of the experiment and the hardware settings are unclear.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}