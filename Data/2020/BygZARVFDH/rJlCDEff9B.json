{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposes to combine energy functions to realize compositionality. This is interesting, and different from previous methods, which use either an explicit vector of factors that is input to a generator function, or object slots that are blended to form an image.\nSpecifically, three operators (logical conjunction, disjunction, and negation) are realized and empirically evaluated through combining energy functions in three different ways.\nExtrapolating concept combinations, continually learning, and concept inference are also evaluated.\n\nThis paper is well motivated, showing compositional generation and inference for images. However, I have some concerns:\n\n1. The experiments on the CelebA dataset are mainly subjective.\n\n2. The equal sign in Eq.(4) should be \\proto.\n\n3. The most serious concern is that although empirical results are promising, I have concern about the correctness that Eq.(6) realizes disjunciton, and Eq.(8) realizes negation.\n\nIt is sensible that Eq.(4) realizes conjuction, according to the idea of Product-of-Expert. Multiplying several energy-based densities reduces to summation of the energies. \n\nFor Eq.(6), the authors ignore the influence of normalizing constants when adding several energy-based densities. The authors seem to assume that the normalizing constants for p(x|c_i) are equal. Justification is needed.\n\nNote that we cannot have :\nlog [0.6* exp(-E(x|c1)) + 0.4* exp(-E(x|c2)) ] will output c1 with probability 0.6 and c2 probability 0.4.\n\nFig. 1 seems to illustrate ideas at first sight, but is not so convinced at second thought.\n\n4. No discussion for the \\alpha in Eq.(8) for concept negation.\n\n5. The description of the baseline joint model in Section 3.4 is missing.\n\n6. For learning EBMs, the following reference is missed, besides (Kim & Bengio, 2016)\nYunfu Song, Zhijian Ou. Learning Neural Random Fields with Inclusive Auxiliary Generators. arxiv 1806.00271, 2018.\n\n--------update after reading the response-----------\nI appreciate the authors' response, but the paper still lacks in sound justification of assuming equal normalizing constants in concept disjunction and concept negation.\nThe claim that the partition functions are similar across dataset and models is mainly empirical (hardly hold in general). The authors' comment (under Figure 16 in A.6) on scaling the model by a suitable temperature to force histogram match in practice makes their reasoning further complicated.\n\nConvincing quantitative experiments on disjunction and negation lacks. It would be better to focus on conjunction by EBMs, which already can makes a good paper, instead of claiming skeptical disjunction and negation by EBMs. A mixture of distribution is more natural to realize disjunction (Review #1 also comments on this).\n\nTherefore, I tend to keep my original score.\n\nMinor: in the updated paper, \"The equal sign in Eq.(4) should be \\proto\" is not fixed.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}