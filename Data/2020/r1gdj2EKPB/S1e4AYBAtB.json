{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Summary: The paper addresses continual learning challenges such as catastrophic forgetting and task-order robustness by introducing a new hybrid algorithm that uses architecture growth as well as parameter regularization where parameters of each layer are decomposed into task-specific and task-private parameters. They also use a simple trick to The authors perform experiments on Split CIFAR100, CIFAR100 Superclass, Omniglot, and a sequence of 3 datasets (SVHN,CIFAR10,CIFAR100). The maximum number of tasks in the experiments is 100 for Omniglot-rotation. The authors show superior performance to EWC (a regularization-based method), P&C (architecture-based method), DEN (architecture-based method), PGN (architecture-based method), RCL (architecture-based method), etc. \n\nPros:\n+ The paper is well-written and has motivated the problem of scalability and forgetting\n+ Proposing a new hybrid approach that benefits from the best of both worlds (maximum usage of the capacity with parameter regularization followed by logarithmic architecture growth at arrival of new task using layer-wise parameter decomposition.\n\nCons that significantly affected my score and resulted in rejecting the paper are as follows:\n\t\n1- Lack of measuring forgetting: \nAuthors indicate in the abstract that \u201ca continual learning model should effectively handle catastrophic forgetting\u201d and reiterate on this on other parts of the paper yet there is no table/figure that shows the initial performance of the model on each task so that readers can compare it with the reported accuracy after being done with all tasks. Having a method with high average accuracy does not necessarily mean it has minimum forgetting. You can use forgetting measurements such as BackWard Transfer, introduced in [1] or forgetting ratio defined in [2] for this assessment. A continual learning paper without proper measurement of forgetting is incomplete.\n\n2 - Large-scale experiment is not convincing:\nAuthors believe scalability has not been addressed well in the literature (page 1&2) and claim it as one of their main contributions and making it crucial to support this claim. However, the experimental setting chosen for this claim is not convincing. Authors have chosen Omniglot-rotation as their longest sequence of tasks with 100 tasks where each task has 12 classes and in each class, there exists 80 images. This will make the total dataset of size 96K images which is still far from being large-scale. While I am aware of the fact that in the current CL literature, the maximum task sequence\u2019s length is only 20 (Split CIFAR100) and I  agree that having an order of magnitude increase in the # of tasks is beneficial,  however, Omniglot is still a toy benchmark and does not serve as a large-scale dataset by only extending it to different random rotations. Moreover, the architecture used for this experiment is LeNet which oversimplifies the problem to address. For incremental learning, I would personally think of ImageNet as a good example and for continual learning of multiple datasets you can consider the existing sequence of 8 tasks benchmarked in [2] and [3] where you can evaluate your method on more realistic images with a total of over 400K images and significant shift in the distributions. As a side note, the key idea behind the proposed method is that this method is able to decompose the parameters into task-specific and task-private whereas in the Omniglot experiment it is not intuitive that what is there between the random rotations that is shared among the task. A more detailed discussion on this would be enlightening. \n\n3 - No standard deviations shown in the results:\nAlthough the results are said to be average over 3 runs, no STD is reported. Given that in the most important experiment of this paper (Omniglot) the difference between Accuracy obtained by PGN and APD is not significant (79.35% vs 81.6%). \nIn the current CL literature, robustness to the order of the tasks is shown by performing multiple permutations of the tasks and reporting average and STD. It is needed that authors show results for this for a fair comparison. \n\n4 - Lack of regularization-based baselines:\nConsidering the fact that the proposed method is a hybrid approach, it is reasonable to compare against both architecture-based and regularization-based approaches. However, most of the baselines are chosen from the former category and EWC is the only baseline for the latter category which is relatively old and has been outperformed by large margins in the past couple of years such as SI [4], VCL[5], HAT [2], PackNet [6], MASS [7], and UCB [3]. \n\nLess major (only to help, and not necessarily part of my decision assessment):\n\nPlease consider explaining connection to prior work (HAT): While the literature review seems comprehensive, authors have missed one important previous work from ICML 2018 [2] called \u201cOvercoming Catastrophic Forgetting with Hard Attention to the Task\u201d or HAT. Both HAT and APD use an attention mechanism to alleviate forgetting. Considering HAT is a very strong baseline, I highly recommend authors provide a comparison with it. It\u2019s an efficient and relatively scalable method that has very small BWT. \nI recommend authors provide their method\u2019s ability for zero-shot transfer or so called forward transfer metric to further support their method.\nHyper parameter tuning: It is also worth mentioning how the tuning process was performed. In continual learning we cannot assume that we have access to all tasks' data, hence authors might want to shed some light on this.\n\nMinor point: On page 8, last paragraph, the authors state that a masked-based pruning technique (Piggyback) is immune to forgetting which is not an accurate statement (Note that PGN is indeed zero-forgetting by definition). All masked-based methods lose some of their performance prior to pruning. While it is correct to say that their post-pruning performance is 100% recoverable by saving the mask, forgetting should be measured with respect to their performance prior to pruning because that is their trade-off to give up accuracy in lieu of freeing space for future tasks.\n\nReferences:\n[1] Lopez-Paz, David, and Marc'Aurelio Ranzato. \"Gradient episodic memory for continual learning.\" Advances in Neural Information Processing Systems. 2017.\n\n[2] Serr\u00e0, J., Sur\u00eds, D., Miron, M. & Karatzoglou, A.. (2018). Overcoming Catastrophic Forgetting with Hard Attention to the Task. Proceedings of the 35th International Conference on Machine Learning, in PMLR 80:4548-4557\n\n[3] Ebrahimi, Sayna, et al. \"Uncertainty-guided Continual Learning with Bayesian Neural Networks.\" arXiv preprint arXiv:1906.02425 (2019).\n\n[4] Zenke, Friedemann, Ben Poole, and Surya Ganguli. \"Continual learning through synaptic intelligence.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017.\n\n[5] Nguyen, Cuong V., et al. \"Variational continual learning.\" arXiv preprint arXiv:1710.10628 (2017).\n\n[6] Mallya, Arun, and Svetlana Lazebnik. \"Packnet: Adding multiple tasks to a single network by iterative pruning.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n\n[7] Aljundi, Rahaf, et al. \"Memory aware synapses: Learning what (not) to forget.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------\nPost-rebuttal response:\n\nThank you for taking the time to go through comments and providing your responses. \n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] In Table 2 (paragraph \u201cContinual learning with heterogeneous datasets\u201d), we have experimental results with heterogeneous datasets, where continual learning models are evaluated on a sequence of different datasets (CIFAR-100, CIFAR-10, SVHN). We agree that experiments with massive datasets will be helpful, but we do not have sufficient time to perform all the experiments during the short rebuttal period. Hence, we only compared against HAT in our revision (Please see Table A.7 and Figure A.10) on the sequence of 8 tasks [2][3] you mentioned. We directly followed all experimental settings on the paper and the code of the authors (https://github.com/joansj/hat). APD (82.42 +- 0.5 %) outperms HAT (80.36 +- 1.2 %) in terms of accuracy. Although HAT shows a marginal forgetting (0.14 %) during training, the models is task-order sensitive (AOPD: 7.95%, MOPD: 23.15%) on difficult sequences of tasks as CIFAR10, CIFAR100, and FaceScrub (Please see Figure A.10) while APD consistently shows a reliable performance with lower OPDs (AOPD: 2.09%, MOPD: 4.40%) regardless of the task order. We will add in all baselines and APD variants in the final version of the paper for this 8-dataset experiment, if it gets accepted.\n\n[Reviewer's response:] while I thank the authors in providing this comparison, I would not call this \"significant outperforming\". According to Table A.7 HAT method achieves  80.36% using the memory needed to store one single network in the memory on which they learn attention masks without using extra memory while APD uses 81% more memory only to achieve 82.42% average accuracy (2.13% increase) which is clearly not a fair comparison to me. Authors should either use the same memory for HAT (using a larger network architecture) or use a smaller memory size for APD and re-evaluate this comparison otherwise it is not conclusive which method is superior. Given the large difference in memory usage I suspect HAT will outperform ADP if given more capacity. While I agree with authors that regularization based approaches can be limited by the number of tasks, in the experimental setting used in this paper, this is not proven to be the case as these methods have not reached their maximum capacity and in fact are still performing strongly well compared to a hybrid approach which is presumably supposed to be better. I appreciate the novelty of the idea of decomposing parameters but it is not clear whether this factorization is actually performed given the high capacity needed to learn these tasks. Therefore, the results are still not convincing to me. \nIn addition,  as also brought up by R1, \"My biggest concern is the choice of baselines. The paper (rightly)  highlights that their work improves over many existing works as they provide a mechanism to \"retroactively update task-adaptive parameters\" for the previous task. But none of their baselines have this mechanism built-in. So while there is a clear advantage with the proposed approach, the comparison is unfair and the baselines should have considered approaches like GEM [0[ and A-GEM[1] while also provide a kind-of retrospective mechanism to correct the weights corresponding to the subsequent tasks. Without such a comparison, it is difficult to comment on the benefits of the approach.\" --> regarding the comparison between APD and HAT in the order-robustness, this also seems as a big concern to me. Maybe I am missing it in the long list of comments and replies, but I am not able to find a clear response from authors in providing a fair assessment without their order robustness constraint mechanism (Eq. 2). This is an auxiliary advantage that only APD is benefitting from and makes the comparison difficult.  It should be either given to all methods or none.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] This is a critical misunderstanding as those results are already provided in the paper.  We already clearly report the performance evolution of several tasks during the course of training in Figure 5 (a)-(c). Figure 5 (a)-(c) clearly show that APD does not suffer from catastrophic forgetting and even improves performance on previously trained task during continual learning (Figure 5 (b)), which is an effect of update on the task-shared parameters. Please see Page 7, \u201cPreventing catastrophic forgetting\u201d paragraph for more detailed discussions. \n\n[Reviewer's response:] I disagree with authors' response regarding Figure 5 being a complete forgetting measurement supported by this sentence in \u201cPreventing catastrophic forgetting\u201d paragraph: \"APD-Nets do not show any sign of catastrophic forgetting\" and showing the performance of only 3 tasks out of 20 in which accuracies are barely readable due to the coarse scale of the figure and more importantly authors do not provide other methods' performance on these tasks. However, I thank authors for providing the comparison with stronger baselines and proper forgetting measurement in A.6 and A.7, I highly recommend swapping Figure 5 with your newly obtained quantitative forgetting measurements shown in A.6 and A.7 in the appendix \n (once fairly compared according to my comment above) as they provide a better support for forgetting avoidance. \n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:] The only hyperparameters required for APD are \\lambda_1 and \\lambda_2, which controls the capacity of the task-adaptive parameters and model drift respectively.  Since there is a trade-off between efficiency (network capacity) and accuracy, the users only need to tune them according to their priority. The model is not sensitive to hyperparameter configurations unless they are in the correct scale, and the details of the hyperparameter configurations are given in A.1. \n\n[Reviewer's response:] Hyper-parameters can be a lot more than just \\lambda_1 and \\lambda_2 in your experiments. Batch size, optimizer's learning rate, weight decay, validation set size, etc are the parameters that are usually left out  in the CL settings and are not explained how they were tuned. As I said before, it is worth explaining what this sentence from A.1 means \"All hyperparameters are determined from a validation set.\" so if this validation is composed of the data from all tasks or they followed some procedure like A-GEM paper in which it is assumed data of only 3 tasks is available in the beginning for tuning purposes.\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n[Authors' response:]  What you mentioned about mask-based methods may be true in general, but not in the case of Piggyback. Piggyback is composed of frozen backbone and task-specific masks, such that the backbone network is fixed without any updates during training, and the model only learns the task-specific pruning masks, which are *stored*, such that we can recover the performance on any previous tasks at any future points. Thus Piggyback does not perform actual pruning, and thus there is no loss of accuracy and forgetting on the previous tasks.\n\n[Reviewer's response:] I disagree with your statement about \"no loss of accuracy\". Restating from Piggyback paper on its page 4: \"The key idea behind our method is to learn to selectively mask the fixed weights of a base network, so as to improve performance on a new task. We achieve this by maintaining a set of real-valued weights that are passed through a deterministic thresholding function to obtain binary masks, that are then applied to existing weights. By updating the real-valued weights through backpropagation, we hope to learn binary masks appropriate for the task at hand.\" \nThis simply means they learn binary mask per task by using a thresholding function and save this mask. However, if you evaluate the model on a given task **prior to making** you obtain a different performance compared to evaluating after masking (this is what you save) where the former is usually higher or sometimes similar to the former because prior to masking, there exist more parameters while by masking some parameters will be 'freed' to be used for future tasks. This difference is what I am referring to as true forgetting and is zero only if evaluation prior and post masking are exactly the same because by storing the learned masks you can only recover the post masking performance. \n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[Authors' response:] The scalability problem we aim to tackle in our work is the scalability to number of tasks. This is because scalability to the size of the network or the number of data instances is basically the problem with generic machine learning and are not the main problem associated with continual learning. Since continual learning models learns on a sequence of tasks, we were more interested in how the existing (expansion-based) continual learning methods and ours behave on large number of tasks, in terms of catastrophic forgetting and network capacity. However, we agree that the term \u2018large-scale continual learning\u2019 may be misleading and have renamed the paragraph to \u2018scalability to large number of tasks\u2019 in the revision. \n\n[Reviewer's response:] I disagree with the first sentence that dataset size is not the main problem associated with CL. It is an important factor that should be considered because dataset size and number/diversity of classes can significantly increase forgetting on early tasks as the distribution shift between the tasks will be significant. I understand the intention of authors and their interest in modeling large sequence of tasks, however introducing the method as scalable is misleading and in addition to the text which is corrected now, should be also corrected in the title of the paper.\n\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nI accept the response for the remaining questions from authors but intend to keep my score. However, I will be happy to update it based on the authors' response to the very first comment above regarding providing a fair comparison in memory size and order-robustness mechanism.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}