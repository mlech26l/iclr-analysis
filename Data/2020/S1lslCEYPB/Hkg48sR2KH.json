{"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #1", "review": "This paper develops variations on the Donsker-Varadhan (DV) lower bound on KL divergence which yields a lower bound on mutual information as a special case.  The paper is primarily theoretical with an emphasis on the case where the witness function f is drawn from an RKHS (a support vector machine).  They discuss, and present experimental results where the feature map of the RKHS is computed by a neural network, although the theoretical results largely do not apply to optimization of the neural network.\n\nI have two complaints.  First, the authors ignore fundamental limitations on the measurement of mutual information from finite samples.  See \"Fundamental Limitations on the Measurement of Mutual Information\" by McAllester and Stratos.  This paper makes the intuitively obvious observation that I(X,Y) <= H(X) and one cannot give meaningful lower bound on H(X) larger than about 2 log N when drawing only N samples --- the best test one can use is the birthday paradox and a non-repetitive sample only guarantees that the entropy is larger than about 2 log N.  This is obvious for discrete distributions but holds also for continuous distributions --- from a finite sample one cannot even tell if the distribution is continuous or discrete.   So meaningful lower bounds for \"high dimensional data\" are simply not possible for feasible samples.  Given this fact, the emphasis needs to be on experimental results.\n\nThe experimental results in this paper are extremely weak. They should be compared to those in \"Learning Representations by Maximizing Mutual Information Across Views\" by Philip Bachman, R Devon Hjelm, William Buchwalter\n\n\nResponse to the author response:\n\nThis was written earlier but there was a mishap when I attempted to submit it and it was not actually submitted until comments were no longer available to the authors so I am putting this in the review.\n\nBounding the ratio of the densities already bounds the mutual information.  In order for the actual mutual information to be large (hundreds of bits) the log density ratios must actually be extreme.  We do believe, presumably, that mutual information in video data can be hundreds of bits.  So I am still not convinced that lower bounds on large quantities of mutual information are meaningful.\n\nAlso, your bound, like the DV bound, involves an empirical estimator $\\frac{1}{N} \\sum_{i=1}^N\\; e^{f(x)}$ of an expectation $E_x e^{f(x)}$.  The true expectation of an exponential seems likely to be dominated by rare extremes of f(x) which contribute exponentially to the expectation.  I see no reason to believe that the empirical estimate is meaningful in a real application such as vision.\n\nRegarding the experiments, I do not have much interest in experiments on synthetic Gaussians.  The most meaningful experiments for me are the pre-training results for CIFAR.  But you seem to be comparing yourself to your own implementations of (weak?) baselines rather than performance numbers reported by, for example, Hjelm et al.  or van den Oord et al. (CPC).\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}