{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "Disclaimer: I was able to read the other reviews and the author\u2019s responses before finalizing this review.\n\nThe paper proposes an easy-to-implement algorithm for DRO on example weights, with the same computational cost as SGD.  The algorithm is based on hardness weighted sampling and links are shown to hard example mining.  Convergence results are shown for (finitely) wide networks.  Additionally, the paper claims to demonstrate the usefulness of the algorithm in deep learning.\n\nI am unable to assess how the convergence results place in the literature, but I believe they motivate the algorithm.  The claims of practical usefulness in deep learning do not seem supported by the provided empirical evidence.\nFor the CIFAR experiments, the ERM baseline does not seem to train - probably due to the choice of learning rate.  This makes it unclear how the proposed algorithm compares.  They claim the algorithm is more robust to the learning rate, so is it possible to train with the original learning rate used for the ERM baseline?  Why was a different learning rate chosen?\n\nIf the experimental results showed an improvement over a properly trained ERM baseline on CIFAR I would lean toward weak accept.\n\nMany state-of-the-art deep learning pipelines do not use plain SGD - for example, the WideResNet you used on CIFAR. How is Algorithm 1 used on these? Do we only make changes to the update on line 24?  Using momentum with nested optimization can introduce instabilities. Perhaps combining momentum with nested optimization of loss weights and parameters is why the baseline does not train?  Maybe you could try an architecture that trained using vanilla SGD, so you can better leverage your theoretical results?\n\nIt would provide evidence of the usefulness of the algorithm if we could take various pipelines and just drop their optimizer updates into algorithm 1 - hopefully without having to spend time re-tuning their optimizer parameters. It would also be nice to see the training loss/accuracy - perhaps over all classes and just cats - in the appendix.\n\n\nThings to improve that did not impact the score:\n\n(Page 1) \u201cin term\u201d -> \u201cin terms\u201d\n(Page 1 & 2) \u201can history\u201d -> \u201ca history\u201d\n(Page 2) \u201cOptmization\u201d -> \u201cOptimization\u201d\n(Page 5) \u201callows to link\u201d -> \u201callows us to link\u201d\n(Page 7) \u201cwe focus at\u201d -> \u201cwe focus on\u201d\n(Page 8) \u201callows to guarantee\u201d -> \u201callows us to guarantee\u201d\n(Page 19) \u201ccontinous\u201d -> \u201ccontinuous", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}