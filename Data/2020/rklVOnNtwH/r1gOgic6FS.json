{"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "** post rebuttal start **\n\nAfter reading reviews and authors' response, I decided not to change my score.\nHowever, I feel that this paper is somewhat under-evaluated initially, so I hope the authors have an opportunity in another venue with their revision.\n\n\nDetailed comments:\n\n1.1. I recommend to add an algorithm box describing the learning scheme. It is not end-to-end learning, so it is hard to catch (and potentially, replicate) the learning part. I am also a bit skeptical about the convergence (with non-zero \\sigma), as Reviewer 2 has a concern about it.\n\n1.3. \"We hypothesized that the value of the uncertainty is different depending on whether the inputs are OOD or in-distribution inputs. The results of the ablation study listed in Table1 demonstrate that this hypothesis is true.\"\n2. \"In order to use the data uncertainty, we used the value of \\sigma.\"\n-> Table 1 proves that your proposal (playing with \\sigma) is effective, but it does not mean that \\sigma is the uncertainty which is only essential component for detecting OOD. I recommend the authors to validate their hypothesis, maybe by conducting more experiments to show that the role of \\mu and \\sigma is as expected. At least, if \\mu is proven to have no effect on OOD detection by some experiment, then it can be a clue.\n\n\nMinor comment: I hope ICLR papers are cited as ICLR papers at least in ICLR submissions, not arXiv preprint.. Alemi's paper is ICLR'17 paper, for example.\n\n** post rebuttal end **\n\n\n\n- Summary: This paper proposes to train an OOD detection model from a portion of modified latent vectors; more specifically, similar to VAE, they assume unimodal Gaussian distributed latent space at each layer and use the collection of standard deviation to train an OOD detector. Experimental results on several OOD benchmarks with different backbone networks show that their method outperforms ODIN (Liang et al., 2017).\n\n\n- Decision and supporting arguments:\nWeak reject.\n\n1. Though the idea of extracting uncertainty is interesting, but I think the motivation and explanation is not enough, so I couldn't find a rationale why we should do this. I have several questions that I couldn't find an answer in the submission, could you answer them?\n1.1. Are the classification loss and OOD detection loss optimized jointly?\n1.2. Is it reasonable to assume unimodal Gaussian distribution over all latent spaces without a carefully designed learning objective? More specifically, to make it learnable, don't you need a learning objective other than the conventional cross-entropy loss, e.g., \"Bayes by backprop\" proposed in the early work (Blundell, 2015)?\n1.3. Why only the standard deviation values are useful for the OOD detection performance? If they are really useful, how the standard deviation values are related to the OOD detection performance?\n\nBlundell et al. Weight Uncertainty in Neural Networks. In ICML, 2015.\n\n2. More ablation study is required to verify the effectiveness of their method. Again, I am not sure why \\mu and \\sigma should be split, and why \\mu should be discarded for the OOD detection part.\n\n3. The architecture design of CNN in Figure 7 also looks arbitrary.\n\n4. Comparison with more state-of-the-art methods is required. ODIN (Liang et al., 2017) is a powerful method but it is somewhat old and many recent works actually combine their method with ODIN for better performance. Why don't you compare the proposed method with the Mahalanobis distance-based classifier (Lee et al., 2018)? They also estimate the uncertainty by measuring the Mahalanobis distance on the feature spaces & combine them for better OOD detection.\n\n\n- Comments:\n1. I couldn't find any statement about the classification accuracy, does the proposed model have a good classification performance as well? Since {a half of the model capacity is spent to split \\mu and \\sigma} and {it should take account of uncertainty in the forward pass}, I am not sure it maintains a good classification performance, compared to the standard classification model with the same capacity.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}