{"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper describes a method to improve the AltQ algorithm (which is typically unstable and inefficient) by using a combination of an Adam optimizer and regularly restarting the internal parameters of the Adam optimizer. The approach is evaluated on both a synthetic problem and on Atari games.\n\nThe core of the approach (simply replacing the optimizer with Adam) is relatively simple and the restarts seem mostly to improve variance rather than return over the vanilla Adam approach. It's hard to see what additional value the convergence analysis provides over the AMSGrad convergence analysis. Especially when the convergence rate appears to be the same for AltQ-AMSGrad and AltQ-AMSGradR for large r. Overall, it seems the approach of using Adam for the optimizer in AltQ seems to be too trivial an improvement and the difference between the Adam with restarts and Adam without restarts also seems to be relatively insignificant when looking at normalized return on Atari. They also both have increasing variance near the end of training in figure 2, much larger than that of DQN. Another downside of this work is that the convergence analysis is done on AMSGrad instead of Adam which the experimental results are based on, why were experiments not done with AltQ-AMSGrad?\n\nOther comments:\nIn section 4.1: \"to help preventing\" -> \"to help prevent\"\nBottom of page 5: \"most well-performed\" -> \"most well-performing\"\nAbove eq 8: \"step stone\" -> \"stepping stone\"\n\n================================================================================================\nUpdate after rebuttal:\n\nThanks for the response. I will stand by my score. I still find it odd that no experiments are done with AltQ-AMSGrad since the convergence analysis was done for this algorithm. After all, the results of AltQ-Adam might be very different to AltQ-AMSGrad, which puts into question how relevant that convergence analysis is to the experimental results.\n\nFinally, I agree with the other reviewers that AltQ is a misleading name for this algorithm and recommend the paper use a standard name for the algorithm.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}